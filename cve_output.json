[
  {
    "text":"void KPasswordDlg::keyPressed( QKeyEvent *e )\n{\n  static bool waitForAuthentication = false;\n  if (!waitForAuthentication) {\n\tswitch ( e->key() )\n\t{\n\t\tcase Key_Backspace:\n\t\t\t{\n\t\t\t\tint len = password.length();\n\t\t\t\tif ( len ) {\n\t\t\t\t\tpassword.truncate( len - 1 );\n\t\t\t\t\tif( stars )\n\t\t\t\t\t\tshowStars();\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase Key_Return:\n            timer.stop();\n\t\t\twaitForAuthentication = true;\n\t\t\tif ( tryPassword() )\n\t\t\t\temit passOk();\n\t\t\telse\n\t\t\t{\n\t\t\t\tlabel->setText( glocale->translate(\"Failed\") );\n\t\t\t\tpassword = \"\";\n\t\t\t\ttimerMode = 1;\n\t\t\t\ttimer.start( 1500, TRUE );\n\t\t\t}\n\t\t\twaitForAuthentication = false;\n\t\t\tbreak;\n\n\t\tcase Key_Escape:\n\t\t\temit passCancel();\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tif ( password.length() < MAX_PASSWORD_LENGTH )\n\t\t\t{\n\t\t\t\tpassword += (char)e->ascii();\n\t\t\t\tif( stars )\n\t\t\t\t\tshowStars();\n\t\t\t\ttimer.changeInterval( 10000 );\n\t\t\t}\n\t}\n  }\n}",
    "vulnerable":0,
    "cve_id":"CVE-1999-0731",
    "description":"[{'lang': 'en', 'value': 'The KDE klock program allows local users to unlock a session using malformed input.'}]"
  },
  {
    "text":"void KPasswordDlg::keyPressed( QKeyEvent *e )\n{\n  static bool waitForAuthentication = false;\n  if (!waitForAuthentication) {\n\tswitch ( e->key() )\n\t{\n\t\tcase Key_Backspace:\n\t\t\t{\n\t\t\t\tint len = password.length();\n\t\t\t\tif ( len ) {\n\t\t\t\t\tpassword.truncate( len - 1 );\n\t\t\t\t\tif( stars )\n\t\t\t\t\t\tshowStars();\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase Key_Return:\n            timer.stop();\n\t\t\twaitForAuthentication = true;\n\t\t\tif ( tryPassword() )\n\t\t\t\temit passOk();\n\t\t\telse\n\t\t\t{\n\t\t\t\tlabel->setText( glocale->translate(\"Failed\") );\n\t\t\t\tpassword = \"\";\n\t\t\t\ttimerMode = 1;\n\t\t\t\ttimer.start( 1500, TRUE );\n\t\t\t}\n\t\t\twaitForAuthentication = false;\n\t\t\tbreak;\n\n\t\tcase Key_Escape:\n\t\t\temit passCancel();\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tif ( password.length() < MAX_PASSWORD_LENGTH )\n\t\t\t{\n\t\t\t\tpassword += (char)e->ascii();\n\t\t\t\tif( stars )\n\t\t\t\t\tshowStars();\n\t\t\t\ttimer.changeInterval( 10000 );\n\t\t\t}\n\t}\n  }\n}",
    "vulnerable":0,
    "cve_id":"CVE-1999-0731",
    "description":"[{'lang': 'en', 'value': 'The KDE klock program allows local users to unlock a session using malformed input.'}]"
  },
  {
    "text":"armv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t\/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t *\/\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t\/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t *\/\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t\/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t *\/\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"armv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t\/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t *\/\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t\/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t *\/\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t\/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t *\/\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"armv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t\/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t *\/\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t\/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t *\/\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t\/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t *\/\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"armv6pmu_handle_irq(int irq_num,\n\t\t    void *dev)\n{\n\tunsigned long pmcr = armv6_pmcr_read();\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\tif (!armv6_pmcr_has_overflowed(pmcr))\n\t\treturn IRQ_NONE;\n\n\tregs = get_irq_regs();\n\n\t\/*\n\t * The interrupts are cleared by writing the overflow flags back to\n\t * the control register. All of the other bits don't have any effect\n\t * if they are rewritten, so write the whole value back.\n\t *\/\n\tarmv6_pmcr_write(pmcr);\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\tfor (idx = 0; idx <= armpmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t\/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t *\/\n\t\tif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx, 1);\n\t\tdata.period = event->hw.last_period;\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, 0, &data, regs))\n\t\t\tarmpmu->disable(hwc, idx);\n\t}\n\n\t\/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t *\/\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"static int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t\/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t *\/\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t\/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t *\/\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t\/*\n\t * Repeat if there is more work to be done:\n\t *\/\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"static int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t\/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t *\/\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t\/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t *\/\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t\/*\n\t * Repeat if there is more work to be done:\n\t *\/\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"static int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t\/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t *\/\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t\/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t *\/\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t\/*\n\t * Repeat if there is more work to be done:\n\t *\/\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"static int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tint bit, loops;\n\tu64 status;\n\tint handled;\n\n\tperf_sample_data_init(&data, 0);\n\n\tcpuc = &__get_cpu_var(cpu_hw_events);\n\n\t\/*\n\t * Some chipsets need to unmask the LVTPC in a particular spot\n\t * inside the nmi handler.  As a result, the unmasking was pushed\n\t * into all the nmi handlers.\n\t *\n\t * This handler doesn't seem to have any issues with the unmasking\n\t * so it was left at the top.\n\t *\/\n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tintel_pmu_disable_all();\n\thandled = intel_pmu_drain_bts_buffer();\n\tstatus = intel_pmu_get_status();\n\tif (!status) {\n\t\tintel_pmu_enable_all(0);\n\t\treturn handled;\n\t}\n\n\tloops = 0;\nagain:\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tWARN_ONCE(1, \"perfevents: irq loop stuck!\\n\");\n\t\tperf_event_print_debug();\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\tintel_pmu_lbr_read();\n\n\t\/*\n\t * PEBS overflow sets bit 62 in the global status register\n\t *\/\n\tif (__test_and_clear_bit(62, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tx86_pmu.drain_pebs(regs);\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tdata.period = event->hw.last_period;\n\n\t\tif (perf_event_overflow(event, 1, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t\/*\n\t * Repeat if there is more work to be done:\n\t *\/\n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tintel_pmu_enable_all(0);\n\treturn handled;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2011-2918",
    "description":"[{'lang': 'en', 'value': 'The Performance Events subsystem in the Linux kernel before 3.1 does not properly handle event overflows associated with PERF_COUNT_SW_CPU_CLOCK events, which allows local users to cause a denial of service (system hang) via a crafted application.'}]"
  },
  {
    "text":"static bool em_syscall_is_enabled(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct x86_emulate_ops *ops = ctxt->ops;\n\tu32 eax, ebx, ecx, edx;\n\n\t\/*\n\t * syscall should always be enabled in longmode - so only become\n\t * vendor specific (cpuid) if other modes are active...\n\t *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn true;\n\n\teax = 0x00000000;\n\tecx = 0x00000000;\n\tif (ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx)) {\n\t\t\/*\n\t\t * Intel (\"GenuineIntel\")\n\t\t * remark: Intel CPUs only support \"syscall\" in 64bit\n\t\t * longmode. Also an 64bit guest with a\n\t\t * 32bit compat-app running will #UD !! While this\n\t\t * behaviour can be fixed (by emulating) into AMD\n\t\t * response - CPUs of AMD can't behave like Intel.\n\t\t *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_GenuineIntel_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_GenuineIntel_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_GenuineIntel_edx)\n\t\t\treturn false;\n\n\t\t\/* AMD (\"AuthenticAMD\") *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_AuthenticAMD_edx)\n\t\t\treturn true;\n\n\t\t\/* AMD (\"AMDisbetter!\") *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_AMDisbetterI_edx)\n\t\t\treturn true;\n\t}\n\n\t\/* default: (not Intel, not AMD), apply Intel's stricter rules... *\/\n\treturn false;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2012-0045",
    "description":"[{'lang': 'en', 'value': 'The em_syscall function in arch\/x86\/kvm\/emulate.c in the KVM implementation in the Linux kernel before 3.2.14 does not properly handle the 0f05 (aka syscall) opcode, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application, as demonstrated by an NASM file.'}]"
  },
  {
    "text":"static bool em_syscall_is_enabled(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct x86_emulate_ops *ops = ctxt->ops;\n\tu32 eax, ebx, ecx, edx;\n\n\t\/*\n\t * syscall should always be enabled in longmode - so only become\n\t * vendor specific (cpuid) if other modes are active...\n\t *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn true;\n\n\teax = 0x00000000;\n\tecx = 0x00000000;\n\tif (ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx)) {\n\t\t\/*\n\t\t * Intel (\"GenuineIntel\")\n\t\t * remark: Intel CPUs only support \"syscall\" in 64bit\n\t\t * longmode. Also an 64bit guest with a\n\t\t * 32bit compat-app running will #UD !! While this\n\t\t * behaviour can be fixed (by emulating) into AMD\n\t\t * response - CPUs of AMD can't behave like Intel.\n\t\t *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_GenuineIntel_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_GenuineIntel_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_GenuineIntel_edx)\n\t\t\treturn false;\n\n\t\t\/* AMD (\"AuthenticAMD\") *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_AuthenticAMD_edx)\n\t\t\treturn true;\n\n\t\t\/* AMD (\"AMDisbetter!\") *\/\n\t\tif (ebx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ebx &&\n\t\t    ecx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ecx &&\n\t\t    edx == X86EMUL_CPUID_VENDOR_AMDisbetterI_edx)\n\t\t\treturn true;\n\t}\n\n\t\/* default: (not Intel, not AMD), apply Intel's stricter rules... *\/\n\treturn false;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2012-0045",
    "description":"[{'lang': 'en', 'value': 'The em_syscall function in arch\/x86\/kvm\/emulate.c in the KVM implementation in the Linux kernel before 3.2.14 does not properly handle the 0f05 (aka syscall) opcode, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application, as demonstrated by an NASM file.'}]"
  },
  {
    "text":"static int em_syscall(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\t\/* syscall is not available in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_ud(ctxt);\n\n\tif (!(em_syscall_is_enabled(ctxt)))\n\t\treturn emulate_ud(ctxt);\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif (!(efer & EFER_SCE))\n\t\treturn emulate_ud(ctxt);\n\n\tops->get_msr(ctxt, MSR_STAR, &msr_data);\n\tmsr_data >>= 32;\n\tcs_sel = (u16)(msr_data & 0xfffc);\n\tss_sel = (u16)(msr_data + 8);\n\n\tif (efer & EFER_LMA) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->regs[VCPU_REGS_RCX] = ctxt->_eip;\n\tif (efer & EFER_LMA) {\n#ifdef CONFIG_X86_64\n\t\tctxt->regs[VCPU_REGS_R11] = ctxt->eflags & ~EFLG_RF;\n\n\t\tops->get_msr(ctxt,\n\t\t\t     ctxt->mode == X86EMUL_MODE_PROT64 ?\n\t\t\t     MSR_LSTAR : MSR_CSTAR, &msr_data);\n\t\tctxt->_eip = msr_data;\n\n\t\tops->get_msr(ctxt, MSR_SYSCALL_MASK, &msr_data);\n\t\tctxt->eflags &= ~(msr_data | EFLG_RF);\n#endif\n\t} else {\n\t\t\/* legacy mode *\/\n\t\tops->get_msr(ctxt, MSR_STAR, &msr_data);\n\t\tctxt->_eip = (u32)msr_data;\n\n\t\tctxt->eflags &= ~(EFLG_VM | EFLG_IF | EFLG_RF);\n\t}\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2012-0045",
    "description":"[{'lang': 'en', 'value': 'The em_syscall function in arch\/x86\/kvm\/emulate.c in the KVM implementation in the Linux kernel before 3.2.14 does not properly handle the 0f05 (aka syscall) opcode, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application, as demonstrated by an NASM file.'}]"
  },
  {
    "text":"static int em_syscall(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\t\/* syscall is not available in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_ud(ctxt);\n\n\tif (!(em_syscall_is_enabled(ctxt)))\n\t\treturn emulate_ud(ctxt);\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif (!(efer & EFER_SCE))\n\t\treturn emulate_ud(ctxt);\n\n\tops->get_msr(ctxt, MSR_STAR, &msr_data);\n\tmsr_data >>= 32;\n\tcs_sel = (u16)(msr_data & 0xfffc);\n\tss_sel = (u16)(msr_data + 8);\n\n\tif (efer & EFER_LMA) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->regs[VCPU_REGS_RCX] = ctxt->_eip;\n\tif (efer & EFER_LMA) {\n#ifdef CONFIG_X86_64\n\t\tctxt->regs[VCPU_REGS_R11] = ctxt->eflags & ~EFLG_RF;\n\n\t\tops->get_msr(ctxt,\n\t\t\t     ctxt->mode == X86EMUL_MODE_PROT64 ?\n\t\t\t     MSR_LSTAR : MSR_CSTAR, &msr_data);\n\t\tctxt->_eip = msr_data;\n\n\t\tops->get_msr(ctxt, MSR_SYSCALL_MASK, &msr_data);\n\t\tctxt->eflags &= ~(msr_data | EFLG_RF);\n#endif\n\t} else {\n\t\t\/* legacy mode *\/\n\t\tops->get_msr(ctxt, MSR_STAR, &msr_data);\n\t\tctxt->_eip = (u32)msr_data;\n\n\t\tctxt->eflags &= ~(EFLG_VM | EFLG_IF | EFLG_RF);\n\t}\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2012-0045",
    "description":"[{'lang': 'en', 'value': 'The em_syscall function in arch\/x86\/kvm\/emulate.c in the KVM implementation in the Linux kernel before 3.2.14 does not properly handle the 0f05 (aka syscall) opcode, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application, as demonstrated by an NASM file.'}]"
  },
  {
    "text":"int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint rc = X86EMUL_CONTINUE;\n\tint saved_dst_type = ctxt->dst.type;\n\n\tctxt->mem_read.pos = 0;\n\n\t\/* LOCK prefix is allowed only with some instructions *\/\n\tif (ctxt->lock_prefix && (!(ctxt->d & Lock) || ctxt->dst.type != OP_MEM)) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & SrcMask) == SrcMemFAddr && ctxt->src.type != OP_MEM) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif (unlikely(ctxt->d &\n\t\t     (No64|Undefined|Sse|Mmx|Intercept|CheckPerm|Priv|Prot|String))) {\n\t\tif ((ctxt->mode == X86EMUL_MODE_PROT64 && (ctxt->d & No64)) ||\n\t\t\t\t(ctxt->d & Undefined)) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (((ctxt->d & (Sse|Mmx)) && ((ops->get_cr(ctxt, 0) & X86_CR0_EM)))\n\t\t    || ((ctxt->d & Sse) && !(ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR))) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif ((ctxt->d & (Sse|Mmx)) && (ops->get_cr(ctxt, 0) & X86_CR0_TS)) {\n\t\t\trc = emulate_nm(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->d & Mmx) {\n\t\t\trc = flush_pending_x87_faults(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\t\/*\n\t\t\t * Now that we know the fpu is exception safe, we can fetch\n\t\t\t * operands from it.\n\t\t\t *\/\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src);\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src2);\n\t\t\tif (!(ctxt->d & Mov))\n\t\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->dst);\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_PRE_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Privileged instruction can be executed only in CPL=0 *\/\n\t\tif ((ctxt->d & Priv) && ops->cpl(ctxt)) {\n\t\t\tif (ctxt->d & PrivUD)\n\t\t\t\trc = emulate_ud(ctxt);\n\t\t\telse\n\t\t\t\trc = emulate_gp(ctxt, 0);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Instruction can only be executed in protected mode *\/\n\t\tif ((ctxt->d & Prot) && ctxt->mode < X86EMUL_MODE_PROT16) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Do instruction specific permission checks *\/\n\t\tif (ctxt->d & CheckPerm) {\n\t\t\trc = ctxt->check_perm(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_POST_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\t\t\/* All REP prefixes have the same first termination condition *\/\n\t\t\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0) {\n\t\t\t\tctxt->eip = ctxt->_eip;\n\t\t\t\tctxt->eflags &= ~EFLG_RF;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((ctxt->src.type == OP_MEM) && !(ctxt->d & NoAccess)) {\n\t\trc = segmented_read(ctxt, ctxt->src.addr.mem,\n\t\t\t\t    ctxt->src.valptr, ctxt->src.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tctxt->src.orig_val64 = ctxt->src.val64;\n\t}\n\n\tif (ctxt->src2.type == OP_MEM) {\n\t\trc = segmented_read(ctxt, ctxt->src2.addr.mem,\n\t\t\t\t    &ctxt->src2.val, ctxt->src2.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & DstMask) == ImplicitOps)\n\t\tgoto special_insn;\n\n\n\tif ((ctxt->dst.type == OP_MEM) && !(ctxt->d & Mov)) {\n\t\t\/* optimisation - avoid slow emulated read if Mov *\/\n\t\trc = segmented_read(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &ctxt->dst.val, ctxt->dst.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tctxt->dst.orig_val = ctxt->dst.val;\n\nspecial_insn:\n\n\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t      X86_ICPT_POST_MEMACCESS);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif (ctxt->rep_prefix && (ctxt->d & String))\n\t\tctxt->eflags |= EFLG_RF;\n\telse\n\t\tctxt->eflags &= ~EFLG_RF;\n\n\tif (ctxt->execute) {\n\t\tif (ctxt->d & Fastop) {\n\t\t\tvoid (*fop)(struct fastop *) = (void *)ctxt->execute;\n\t\t\trc = fastop(ctxt, fop);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\tgoto writeback;\n\t\t}\n\t\trc = ctxt->execute(ctxt);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tgoto writeback;\n\t}\n\n\tif (ctxt->opcode_len == 2)\n\t\tgoto twobyte_insn;\n\telse if (ctxt->opcode_len == 3)\n\t\tgoto threebyte_insn;\n\n\tswitch (ctxt->b) {\n\tcase 0x63:\t\t\/* movsxd *\/\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tgoto cannot_emulate;\n\t\tctxt->dst.val = (s32) ctxt->src.val;\n\t\tbreak;\n\tcase 0x70 ... 0x7f: \/* jcc (short) *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x8d: \/* lea r16\/r32, m *\/\n\t\tctxt->dst.val = ctxt->src.addr.mem.ea;\n\t\tbreak;\n\tcase 0x90 ... 0x97: \/* nop \/ xchg reg, rax *\/\n\t\tif (ctxt->dst.addr.reg == reg_rmw(ctxt, VCPU_REGS_RAX))\n\t\t\tctxt->dst.type = OP_NONE;\n\t\telse\n\t\t\trc = em_xchg(ctxt);\n\t\tbreak;\n\tcase 0x98: \/* cbw\/cwde\/cdqe *\/\n\t\tswitch (ctxt->op_bytes) {\n\t\tcase 2: ctxt->dst.val = (s8)ctxt->dst.val; break;\n\t\tcase 4: ctxt->dst.val = (s16)ctxt->dst.val; break;\n\t\tcase 8: ctxt->dst.val = (s32)ctxt->dst.val; break;\n\t\t}\n\t\tbreak;\n\tcase 0xcc:\t\t\/* int3 *\/\n\t\trc = emulate_int(ctxt, 3);\n\t\tbreak;\n\tcase 0xcd:\t\t\/* int n *\/\n\t\trc = emulate_int(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0xce:\t\t\/* into *\/\n\t\tif (ctxt->eflags & EFLG_OF)\n\t\t\trc = emulate_int(ctxt, 4);\n\t\tbreak;\n\tcase 0xe9: \/* jmp rel *\/\n\tcase 0xeb: \/* jmp rel short *\/\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tctxt->dst.type = OP_NONE; \/* Disable writeback. *\/\n\t\tbreak;\n\tcase 0xf4:              \/* hlt *\/\n\t\tctxt->ops->halt(ctxt);\n\t\tbreak;\n\tcase 0xf5:\t\/* cmc *\/\n\t\t\/* complement carry flag from eflags reg *\/\n\t\tctxt->eflags ^= EFLG_CF;\n\t\tbreak;\n\tcase 0xf8: \/* clc *\/\n\t\tctxt->eflags &= ~EFLG_CF;\n\t\tbreak;\n\tcase 0xf9: \/* stc *\/\n\t\tctxt->eflags |= EFLG_CF;\n\t\tbreak;\n\tcase 0xfc: \/* cld *\/\n\t\tctxt->eflags &= ~EFLG_DF;\n\t\tbreak;\n\tcase 0xfd: \/* std *\/\n\t\tctxt->eflags |= EFLG_DF;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\nwriteback:\n\tif (ctxt->d & SrcWrite) {\n\t\tBUG_ON(ctxt->src.type == OP_MEM || ctxt->src.type == OP_MEM_STR);\n\t\trc = writeback(ctxt, &ctxt->src);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tif (!(ctxt->d & NoWrite)) {\n\t\trc = writeback(ctxt, &ctxt->dst);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\t\/*\n\t * restore dst type in case the decoding will be reused\n\t * (happens for string instruction )\n\t *\/\n\tctxt->dst.type = saved_dst_type;\n\n\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RSI, &ctxt->src);\n\n\tif ((ctxt->d & DstMask) == DstDI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RDI, &ctxt->dst);\n\n\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\tunsigned int count;\n\t\tstruct read_cache *r = &ctxt->io_read;\n\t\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\t\tcount = ctxt->src.count;\n\t\telse\n\t\t\tcount = ctxt->dst.count;\n\t\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX),\n\t\t\t\t-count);\n\n\t\tif (!string_insn_completed(ctxt)) {\n\t\t\t\/*\n\t\t\t * Re-enter guest when pio read ahead buffer is empty\n\t\t\t * or, if it is not used, after each 1024 iteration.\n\t\t\t *\/\n\t\t\tif ((r->end != 0 || reg_read(ctxt, VCPU_REGS_RCX) & 0x3ff) &&\n\t\t\t    (r->end == 0 || r->end != r->pos)) {\n\t\t\t\t\/*\n\t\t\t\t * Reset read cache. Usually happens before\n\t\t\t\t * decode, but since instruction is restarted\n\t\t\t\t * we have to do it here.\n\t\t\t\t *\/\n\t\t\t\tctxt->mem_read.end = 0;\n\t\t\t\twriteback_registers(ctxt);\n\t\t\t\treturn EMULATION_RESTART;\n\t\t\t}\n\t\t\tgoto done; \/* skip rip writeback *\/\n\t\t}\n\t\tctxt->eflags &= ~EFLG_RF;\n\t}\n\n\tctxt->eip = ctxt->_eip;\n\ndone:\n\tif (rc == X86EMUL_PROPAGATE_FAULT) {\n\t\tWARN_ON(ctxt->exception.vector > 0x1f);\n\t\tctxt->have_exception = true;\n\t}\n\tif (rc == X86EMUL_INTERCEPTED)\n\t\treturn EMULATION_INTERCEPTED;\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\twriteback_registers(ctxt);\n\n\treturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\n\ntwobyte_insn:\n\tswitch (ctxt->b) {\n\tcase 0x09:\t\t\/* wbinvd *\/\n\t\t(ctxt->ops->wbinvd)(ctxt);\n\t\tbreak;\n\tcase 0x08:\t\t\/* invd *\/\n\tcase 0x0d:\t\t\/* GrpP (prefetch) *\/\n\tcase 0x18:\t\t\/* Grp16 (prefetch\/nop) *\/\n\tcase 0x1f:\t\t\/* nop *\/\n\t\tbreak;\n\tcase 0x20: \/* mov cr, reg *\/\n\t\tctxt->dst.val = ops->get_cr(ctxt, ctxt->modrm_reg);\n\t\tbreak;\n\tcase 0x21: \/* mov from dr to reg *\/\n\t\tops->get_dr(ctxt, ctxt->modrm_reg, &ctxt->dst.val);\n\t\tbreak;\n\tcase 0x40 ... 0x4f:\t\/* cmov *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tctxt->dst.val = ctxt->src.val;\n\t\telse if (ctxt->mode != X86EMUL_MODE_PROT64 ||\n\t\t\t ctxt->op_bytes != 4)\n\t\t\tctxt->dst.type = OP_NONE; \/* no writeback *\/\n\t\tbreak;\n\tcase 0x80 ... 0x8f: \/* jnz rel, etc*\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x90 ... 0x9f:     \/* setcc r\/m8 *\/\n\t\tctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);\n\t\tbreak;\n\tcase 0xae:              \/* clflush *\/\n\t\tbreak;\n\tcase 0xb6 ... 0xb7:\t\/* movzx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val\n\t\t\t\t\t\t       : (u16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xbe ... 0xbf:\t\/* movsx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (s8) ctxt->src.val :\n\t\t\t\t\t\t\t(s16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xc3:\t\t\/* movnti *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->op_bytes == 8) ? (u64) ctxt->src.val :\n\t\t\t\t\t\t\t(u32) ctxt->src.val;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\nthreebyte_insn:\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tgoto writeback;\n\ncannot_emulate:\n\treturn EMULATION_FAILED;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint rc = X86EMUL_CONTINUE;\n\tint saved_dst_type = ctxt->dst.type;\n\n\tctxt->mem_read.pos = 0;\n\n\t\/* LOCK prefix is allowed only with some instructions *\/\n\tif (ctxt->lock_prefix && (!(ctxt->d & Lock) || ctxt->dst.type != OP_MEM)) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & SrcMask) == SrcMemFAddr && ctxt->src.type != OP_MEM) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif (unlikely(ctxt->d &\n\t\t     (No64|Undefined|Sse|Mmx|Intercept|CheckPerm|Priv|Prot|String))) {\n\t\tif ((ctxt->mode == X86EMUL_MODE_PROT64 && (ctxt->d & No64)) ||\n\t\t\t\t(ctxt->d & Undefined)) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (((ctxt->d & (Sse|Mmx)) && ((ops->get_cr(ctxt, 0) & X86_CR0_EM)))\n\t\t    || ((ctxt->d & Sse) && !(ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR))) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif ((ctxt->d & (Sse|Mmx)) && (ops->get_cr(ctxt, 0) & X86_CR0_TS)) {\n\t\t\trc = emulate_nm(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->d & Mmx) {\n\t\t\trc = flush_pending_x87_faults(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\t\/*\n\t\t\t * Now that we know the fpu is exception safe, we can fetch\n\t\t\t * operands from it.\n\t\t\t *\/\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src);\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src2);\n\t\t\tif (!(ctxt->d & Mov))\n\t\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->dst);\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_PRE_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Privileged instruction can be executed only in CPL=0 *\/\n\t\tif ((ctxt->d & Priv) && ops->cpl(ctxt)) {\n\t\t\tif (ctxt->d & PrivUD)\n\t\t\t\trc = emulate_ud(ctxt);\n\t\t\telse\n\t\t\t\trc = emulate_gp(ctxt, 0);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Instruction can only be executed in protected mode *\/\n\t\tif ((ctxt->d & Prot) && ctxt->mode < X86EMUL_MODE_PROT16) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Do instruction specific permission checks *\/\n\t\tif (ctxt->d & CheckPerm) {\n\t\t\trc = ctxt->check_perm(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_POST_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\t\t\/* All REP prefixes have the same first termination condition *\/\n\t\t\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0) {\n\t\t\t\tctxt->eip = ctxt->_eip;\n\t\t\t\tctxt->eflags &= ~EFLG_RF;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((ctxt->src.type == OP_MEM) && !(ctxt->d & NoAccess)) {\n\t\trc = segmented_read(ctxt, ctxt->src.addr.mem,\n\t\t\t\t    ctxt->src.valptr, ctxt->src.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tctxt->src.orig_val64 = ctxt->src.val64;\n\t}\n\n\tif (ctxt->src2.type == OP_MEM) {\n\t\trc = segmented_read(ctxt, ctxt->src2.addr.mem,\n\t\t\t\t    &ctxt->src2.val, ctxt->src2.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & DstMask) == ImplicitOps)\n\t\tgoto special_insn;\n\n\n\tif ((ctxt->dst.type == OP_MEM) && !(ctxt->d & Mov)) {\n\t\t\/* optimisation - avoid slow emulated read if Mov *\/\n\t\trc = segmented_read(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &ctxt->dst.val, ctxt->dst.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tctxt->dst.orig_val = ctxt->dst.val;\n\nspecial_insn:\n\n\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t      X86_ICPT_POST_MEMACCESS);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif (ctxt->rep_prefix && (ctxt->d & String))\n\t\tctxt->eflags |= EFLG_RF;\n\telse\n\t\tctxt->eflags &= ~EFLG_RF;\n\n\tif (ctxt->execute) {\n\t\tif (ctxt->d & Fastop) {\n\t\t\tvoid (*fop)(struct fastop *) = (void *)ctxt->execute;\n\t\t\trc = fastop(ctxt, fop);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\tgoto writeback;\n\t\t}\n\t\trc = ctxt->execute(ctxt);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tgoto writeback;\n\t}\n\n\tif (ctxt->opcode_len == 2)\n\t\tgoto twobyte_insn;\n\telse if (ctxt->opcode_len == 3)\n\t\tgoto threebyte_insn;\n\n\tswitch (ctxt->b) {\n\tcase 0x63:\t\t\/* movsxd *\/\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tgoto cannot_emulate;\n\t\tctxt->dst.val = (s32) ctxt->src.val;\n\t\tbreak;\n\tcase 0x70 ... 0x7f: \/* jcc (short) *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x8d: \/* lea r16\/r32, m *\/\n\t\tctxt->dst.val = ctxt->src.addr.mem.ea;\n\t\tbreak;\n\tcase 0x90 ... 0x97: \/* nop \/ xchg reg, rax *\/\n\t\tif (ctxt->dst.addr.reg == reg_rmw(ctxt, VCPU_REGS_RAX))\n\t\t\tctxt->dst.type = OP_NONE;\n\t\telse\n\t\t\trc = em_xchg(ctxt);\n\t\tbreak;\n\tcase 0x98: \/* cbw\/cwde\/cdqe *\/\n\t\tswitch (ctxt->op_bytes) {\n\t\tcase 2: ctxt->dst.val = (s8)ctxt->dst.val; break;\n\t\tcase 4: ctxt->dst.val = (s16)ctxt->dst.val; break;\n\t\tcase 8: ctxt->dst.val = (s32)ctxt->dst.val; break;\n\t\t}\n\t\tbreak;\n\tcase 0xcc:\t\t\/* int3 *\/\n\t\trc = emulate_int(ctxt, 3);\n\t\tbreak;\n\tcase 0xcd:\t\t\/* int n *\/\n\t\trc = emulate_int(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0xce:\t\t\/* into *\/\n\t\tif (ctxt->eflags & EFLG_OF)\n\t\t\trc = emulate_int(ctxt, 4);\n\t\tbreak;\n\tcase 0xe9: \/* jmp rel *\/\n\tcase 0xeb: \/* jmp rel short *\/\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tctxt->dst.type = OP_NONE; \/* Disable writeback. *\/\n\t\tbreak;\n\tcase 0xf4:              \/* hlt *\/\n\t\tctxt->ops->halt(ctxt);\n\t\tbreak;\n\tcase 0xf5:\t\/* cmc *\/\n\t\t\/* complement carry flag from eflags reg *\/\n\t\tctxt->eflags ^= EFLG_CF;\n\t\tbreak;\n\tcase 0xf8: \/* clc *\/\n\t\tctxt->eflags &= ~EFLG_CF;\n\t\tbreak;\n\tcase 0xf9: \/* stc *\/\n\t\tctxt->eflags |= EFLG_CF;\n\t\tbreak;\n\tcase 0xfc: \/* cld *\/\n\t\tctxt->eflags &= ~EFLG_DF;\n\t\tbreak;\n\tcase 0xfd: \/* std *\/\n\t\tctxt->eflags |= EFLG_DF;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\nwriteback:\n\tif (ctxt->d & SrcWrite) {\n\t\tBUG_ON(ctxt->src.type == OP_MEM || ctxt->src.type == OP_MEM_STR);\n\t\trc = writeback(ctxt, &ctxt->src);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tif (!(ctxt->d & NoWrite)) {\n\t\trc = writeback(ctxt, &ctxt->dst);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\t\/*\n\t * restore dst type in case the decoding will be reused\n\t * (happens for string instruction )\n\t *\/\n\tctxt->dst.type = saved_dst_type;\n\n\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RSI, &ctxt->src);\n\n\tif ((ctxt->d & DstMask) == DstDI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RDI, &ctxt->dst);\n\n\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\tunsigned int count;\n\t\tstruct read_cache *r = &ctxt->io_read;\n\t\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\t\tcount = ctxt->src.count;\n\t\telse\n\t\t\tcount = ctxt->dst.count;\n\t\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX),\n\t\t\t\t-count);\n\n\t\tif (!string_insn_completed(ctxt)) {\n\t\t\t\/*\n\t\t\t * Re-enter guest when pio read ahead buffer is empty\n\t\t\t * or, if it is not used, after each 1024 iteration.\n\t\t\t *\/\n\t\t\tif ((r->end != 0 || reg_read(ctxt, VCPU_REGS_RCX) & 0x3ff) &&\n\t\t\t    (r->end == 0 || r->end != r->pos)) {\n\t\t\t\t\/*\n\t\t\t\t * Reset read cache. Usually happens before\n\t\t\t\t * decode, but since instruction is restarted\n\t\t\t\t * we have to do it here.\n\t\t\t\t *\/\n\t\t\t\tctxt->mem_read.end = 0;\n\t\t\t\twriteback_registers(ctxt);\n\t\t\t\treturn EMULATION_RESTART;\n\t\t\t}\n\t\t\tgoto done; \/* skip rip writeback *\/\n\t\t}\n\t\tctxt->eflags &= ~EFLG_RF;\n\t}\n\n\tctxt->eip = ctxt->_eip;\n\ndone:\n\tif (rc == X86EMUL_PROPAGATE_FAULT) {\n\t\tWARN_ON(ctxt->exception.vector > 0x1f);\n\t\tctxt->have_exception = true;\n\t}\n\tif (rc == X86EMUL_INTERCEPTED)\n\t\treturn EMULATION_INTERCEPTED;\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\twriteback_registers(ctxt);\n\n\treturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\n\ntwobyte_insn:\n\tswitch (ctxt->b) {\n\tcase 0x09:\t\t\/* wbinvd *\/\n\t\t(ctxt->ops->wbinvd)(ctxt);\n\t\tbreak;\n\tcase 0x08:\t\t\/* invd *\/\n\tcase 0x0d:\t\t\/* GrpP (prefetch) *\/\n\tcase 0x18:\t\t\/* Grp16 (prefetch\/nop) *\/\n\tcase 0x1f:\t\t\/* nop *\/\n\t\tbreak;\n\tcase 0x20: \/* mov cr, reg *\/\n\t\tctxt->dst.val = ops->get_cr(ctxt, ctxt->modrm_reg);\n\t\tbreak;\n\tcase 0x21: \/* mov from dr to reg *\/\n\t\tops->get_dr(ctxt, ctxt->modrm_reg, &ctxt->dst.val);\n\t\tbreak;\n\tcase 0x40 ... 0x4f:\t\/* cmov *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tctxt->dst.val = ctxt->src.val;\n\t\telse if (ctxt->mode != X86EMUL_MODE_PROT64 ||\n\t\t\t ctxt->op_bytes != 4)\n\t\t\tctxt->dst.type = OP_NONE; \/* no writeback *\/\n\t\tbreak;\n\tcase 0x80 ... 0x8f: \/* jnz rel, etc*\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tjmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x90 ... 0x9f:     \/* setcc r\/m8 *\/\n\t\tctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);\n\t\tbreak;\n\tcase 0xae:              \/* clflush *\/\n\t\tbreak;\n\tcase 0xb6 ... 0xb7:\t\/* movzx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val\n\t\t\t\t\t\t       : (u16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xbe ... 0xbf:\t\/* movsx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (s8) ctxt->src.val :\n\t\t\t\t\t\t\t(s16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xc3:\t\t\/* movnti *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->op_bytes == 8) ? (u64) ctxt->src.val :\n\t\t\t\t\t\t\t(u32) ctxt->src.val;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\nthreebyte_insn:\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tgoto writeback;\n\ncannot_emulate:\n\treturn EMULATION_FAILED;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint rc = X86EMUL_CONTINUE;\n\tint saved_dst_type = ctxt->dst.type;\n\n\tctxt->mem_read.pos = 0;\n\n\t\/* LOCK prefix is allowed only with some instructions *\/\n\tif (ctxt->lock_prefix && (!(ctxt->d & Lock) || ctxt->dst.type != OP_MEM)) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & SrcMask) == SrcMemFAddr && ctxt->src.type != OP_MEM) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif (unlikely(ctxt->d &\n\t\t     (No64|Undefined|Sse|Mmx|Intercept|CheckPerm|Priv|Prot|String))) {\n\t\tif ((ctxt->mode == X86EMUL_MODE_PROT64 && (ctxt->d & No64)) ||\n\t\t\t\t(ctxt->d & Undefined)) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (((ctxt->d & (Sse|Mmx)) && ((ops->get_cr(ctxt, 0) & X86_CR0_EM)))\n\t\t    || ((ctxt->d & Sse) && !(ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR))) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif ((ctxt->d & (Sse|Mmx)) && (ops->get_cr(ctxt, 0) & X86_CR0_TS)) {\n\t\t\trc = emulate_nm(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->d & Mmx) {\n\t\t\trc = flush_pending_x87_faults(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\t\/*\n\t\t\t * Now that we know the fpu is exception safe, we can fetch\n\t\t\t * operands from it.\n\t\t\t *\/\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src);\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src2);\n\t\t\tif (!(ctxt->d & Mov))\n\t\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->dst);\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_PRE_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Privileged instruction can be executed only in CPL=0 *\/\n\t\tif ((ctxt->d & Priv) && ops->cpl(ctxt)) {\n\t\t\tif (ctxt->d & PrivUD)\n\t\t\t\trc = emulate_ud(ctxt);\n\t\t\telse\n\t\t\t\trc = emulate_gp(ctxt, 0);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Instruction can only be executed in protected mode *\/\n\t\tif ((ctxt->d & Prot) && ctxt->mode < X86EMUL_MODE_PROT16) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Do instruction specific permission checks *\/\n\t\tif (ctxt->d & CheckPerm) {\n\t\t\trc = ctxt->check_perm(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_POST_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\t\t\/* All REP prefixes have the same first termination condition *\/\n\t\t\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0) {\n\t\t\t\tctxt->eip = ctxt->_eip;\n\t\t\t\tctxt->eflags &= ~EFLG_RF;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((ctxt->src.type == OP_MEM) && !(ctxt->d & NoAccess)) {\n\t\trc = segmented_read(ctxt, ctxt->src.addr.mem,\n\t\t\t\t    ctxt->src.valptr, ctxt->src.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tctxt->src.orig_val64 = ctxt->src.val64;\n\t}\n\n\tif (ctxt->src2.type == OP_MEM) {\n\t\trc = segmented_read(ctxt, ctxt->src2.addr.mem,\n\t\t\t\t    &ctxt->src2.val, ctxt->src2.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & DstMask) == ImplicitOps)\n\t\tgoto special_insn;\n\n\n\tif ((ctxt->dst.type == OP_MEM) && !(ctxt->d & Mov)) {\n\t\t\/* optimisation - avoid slow emulated read if Mov *\/\n\t\trc = segmented_read(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &ctxt->dst.val, ctxt->dst.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tctxt->dst.orig_val = ctxt->dst.val;\n\nspecial_insn:\n\n\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t      X86_ICPT_POST_MEMACCESS);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif (ctxt->rep_prefix && (ctxt->d & String))\n\t\tctxt->eflags |= EFLG_RF;\n\telse\n\t\tctxt->eflags &= ~EFLG_RF;\n\n\tif (ctxt->execute) {\n\t\tif (ctxt->d & Fastop) {\n\t\t\tvoid (*fop)(struct fastop *) = (void *)ctxt->execute;\n\t\t\trc = fastop(ctxt, fop);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\tgoto writeback;\n\t\t}\n\t\trc = ctxt->execute(ctxt);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tgoto writeback;\n\t}\n\n\tif (ctxt->opcode_len == 2)\n\t\tgoto twobyte_insn;\n\telse if (ctxt->opcode_len == 3)\n\t\tgoto threebyte_insn;\n\n\tswitch (ctxt->b) {\n\tcase 0x63:\t\t\/* movsxd *\/\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tgoto cannot_emulate;\n\t\tctxt->dst.val = (s32) ctxt->src.val;\n\t\tbreak;\n\tcase 0x70 ... 0x7f: \/* jcc (short) *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x8d: \/* lea r16\/r32, m *\/\n\t\tctxt->dst.val = ctxt->src.addr.mem.ea;\n\t\tbreak;\n\tcase 0x90 ... 0x97: \/* nop \/ xchg reg, rax *\/\n\t\tif (ctxt->dst.addr.reg == reg_rmw(ctxt, VCPU_REGS_RAX))\n\t\t\tctxt->dst.type = OP_NONE;\n\t\telse\n\t\t\trc = em_xchg(ctxt);\n\t\tbreak;\n\tcase 0x98: \/* cbw\/cwde\/cdqe *\/\n\t\tswitch (ctxt->op_bytes) {\n\t\tcase 2: ctxt->dst.val = (s8)ctxt->dst.val; break;\n\t\tcase 4: ctxt->dst.val = (s16)ctxt->dst.val; break;\n\t\tcase 8: ctxt->dst.val = (s32)ctxt->dst.val; break;\n\t\t}\n\t\tbreak;\n\tcase 0xcc:\t\t\/* int3 *\/\n\t\trc = emulate_int(ctxt, 3);\n\t\tbreak;\n\tcase 0xcd:\t\t\/* int n *\/\n\t\trc = emulate_int(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0xce:\t\t\/* into *\/\n\t\tif (ctxt->eflags & EFLG_OF)\n\t\t\trc = emulate_int(ctxt, 4);\n\t\tbreak;\n\tcase 0xe9: \/* jmp rel *\/\n\tcase 0xeb: \/* jmp rel short *\/\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tctxt->dst.type = OP_NONE; \/* Disable writeback. *\/\n\t\tbreak;\n\tcase 0xf4:              \/* hlt *\/\n\t\tctxt->ops->halt(ctxt);\n\t\tbreak;\n\tcase 0xf5:\t\/* cmc *\/\n\t\t\/* complement carry flag from eflags reg *\/\n\t\tctxt->eflags ^= EFLG_CF;\n\t\tbreak;\n\tcase 0xf8: \/* clc *\/\n\t\tctxt->eflags &= ~EFLG_CF;\n\t\tbreak;\n\tcase 0xf9: \/* stc *\/\n\t\tctxt->eflags |= EFLG_CF;\n\t\tbreak;\n\tcase 0xfc: \/* cld *\/\n\t\tctxt->eflags &= ~EFLG_DF;\n\t\tbreak;\n\tcase 0xfd: \/* std *\/\n\t\tctxt->eflags |= EFLG_DF;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\nwriteback:\n\tif (ctxt->d & SrcWrite) {\n\t\tBUG_ON(ctxt->src.type == OP_MEM || ctxt->src.type == OP_MEM_STR);\n\t\trc = writeback(ctxt, &ctxt->src);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tif (!(ctxt->d & NoWrite)) {\n\t\trc = writeback(ctxt, &ctxt->dst);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\t\/*\n\t * restore dst type in case the decoding will be reused\n\t * (happens for string instruction )\n\t *\/\n\tctxt->dst.type = saved_dst_type;\n\n\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RSI, &ctxt->src);\n\n\tif ((ctxt->d & DstMask) == DstDI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RDI, &ctxt->dst);\n\n\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\tunsigned int count;\n\t\tstruct read_cache *r = &ctxt->io_read;\n\t\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\t\tcount = ctxt->src.count;\n\t\telse\n\t\t\tcount = ctxt->dst.count;\n\t\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX),\n\t\t\t\t-count);\n\n\t\tif (!string_insn_completed(ctxt)) {\n\t\t\t\/*\n\t\t\t * Re-enter guest when pio read ahead buffer is empty\n\t\t\t * or, if it is not used, after each 1024 iteration.\n\t\t\t *\/\n\t\t\tif ((r->end != 0 || reg_read(ctxt, VCPU_REGS_RCX) & 0x3ff) &&\n\t\t\t    (r->end == 0 || r->end != r->pos)) {\n\t\t\t\t\/*\n\t\t\t\t * Reset read cache. Usually happens before\n\t\t\t\t * decode, but since instruction is restarted\n\t\t\t\t * we have to do it here.\n\t\t\t\t *\/\n\t\t\t\tctxt->mem_read.end = 0;\n\t\t\t\twriteback_registers(ctxt);\n\t\t\t\treturn EMULATION_RESTART;\n\t\t\t}\n\t\t\tgoto done; \/* skip rip writeback *\/\n\t\t}\n\t\tctxt->eflags &= ~EFLG_RF;\n\t}\n\n\tctxt->eip = ctxt->_eip;\n\ndone:\n\tif (rc == X86EMUL_PROPAGATE_FAULT) {\n\t\tWARN_ON(ctxt->exception.vector > 0x1f);\n\t\tctxt->have_exception = true;\n\t}\n\tif (rc == X86EMUL_INTERCEPTED)\n\t\treturn EMULATION_INTERCEPTED;\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\twriteback_registers(ctxt);\n\n\treturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\n\ntwobyte_insn:\n\tswitch (ctxt->b) {\n\tcase 0x09:\t\t\/* wbinvd *\/\n\t\t(ctxt->ops->wbinvd)(ctxt);\n\t\tbreak;\n\tcase 0x08:\t\t\/* invd *\/\n\tcase 0x0d:\t\t\/* GrpP (prefetch) *\/\n\tcase 0x18:\t\t\/* Grp16 (prefetch\/nop) *\/\n\tcase 0x1f:\t\t\/* nop *\/\n\t\tbreak;\n\tcase 0x20: \/* mov cr, reg *\/\n\t\tctxt->dst.val = ops->get_cr(ctxt, ctxt->modrm_reg);\n\t\tbreak;\n\tcase 0x21: \/* mov from dr to reg *\/\n\t\tops->get_dr(ctxt, ctxt->modrm_reg, &ctxt->dst.val);\n\t\tbreak;\n\tcase 0x40 ... 0x4f:\t\/* cmov *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tctxt->dst.val = ctxt->src.val;\n\t\telse if (ctxt->mode != X86EMUL_MODE_PROT64 ||\n\t\t\t ctxt->op_bytes != 4)\n\t\t\tctxt->dst.type = OP_NONE; \/* no writeback *\/\n\t\tbreak;\n\tcase 0x80 ... 0x8f: \/* jnz rel, etc*\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x90 ... 0x9f:     \/* setcc r\/m8 *\/\n\t\tctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);\n\t\tbreak;\n\tcase 0xae:              \/* clflush *\/\n\t\tbreak;\n\tcase 0xb6 ... 0xb7:\t\/* movzx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val\n\t\t\t\t\t\t       : (u16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xbe ... 0xbf:\t\/* movsx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (s8) ctxt->src.val :\n\t\t\t\t\t\t\t(s16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xc3:\t\t\/* movnti *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->op_bytes == 8) ? (u64) ctxt->src.val :\n\t\t\t\t\t\t\t(u32) ctxt->src.val;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\nthreebyte_insn:\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tgoto writeback;\n\ncannot_emulate:\n\treturn EMULATION_FAILED;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint rc = X86EMUL_CONTINUE;\n\tint saved_dst_type = ctxt->dst.type;\n\n\tctxt->mem_read.pos = 0;\n\n\t\/* LOCK prefix is allowed only with some instructions *\/\n\tif (ctxt->lock_prefix && (!(ctxt->d & Lock) || ctxt->dst.type != OP_MEM)) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & SrcMask) == SrcMemFAddr && ctxt->src.type != OP_MEM) {\n\t\trc = emulate_ud(ctxt);\n\t\tgoto done;\n\t}\n\n\tif (unlikely(ctxt->d &\n\t\t     (No64|Undefined|Sse|Mmx|Intercept|CheckPerm|Priv|Prot|String))) {\n\t\tif ((ctxt->mode == X86EMUL_MODE_PROT64 && (ctxt->d & No64)) ||\n\t\t\t\t(ctxt->d & Undefined)) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (((ctxt->d & (Sse|Mmx)) && ((ops->get_cr(ctxt, 0) & X86_CR0_EM)))\n\t\t    || ((ctxt->d & Sse) && !(ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR))) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif ((ctxt->d & (Sse|Mmx)) && (ops->get_cr(ctxt, 0) & X86_CR0_TS)) {\n\t\t\trc = emulate_nm(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->d & Mmx) {\n\t\t\trc = flush_pending_x87_faults(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\t\/*\n\t\t\t * Now that we know the fpu is exception safe, we can fetch\n\t\t\t * operands from it.\n\t\t\t *\/\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src);\n\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->src2);\n\t\t\tif (!(ctxt->d & Mov))\n\t\t\t\tfetch_possible_mmx_operand(ctxt, &ctxt->dst);\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_PRE_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Privileged instruction can be executed only in CPL=0 *\/\n\t\tif ((ctxt->d & Priv) && ops->cpl(ctxt)) {\n\t\t\tif (ctxt->d & PrivUD)\n\t\t\t\trc = emulate_ud(ctxt);\n\t\t\telse\n\t\t\t\trc = emulate_gp(ctxt, 0);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Instruction can only be executed in protected mode *\/\n\t\tif ((ctxt->d & Prot) && ctxt->mode < X86EMUL_MODE_PROT16) {\n\t\t\trc = emulate_ud(ctxt);\n\t\t\tgoto done;\n\t\t}\n\n\t\t\/* Do instruction specific permission checks *\/\n\t\tif (ctxt->d & CheckPerm) {\n\t\t\trc = ctxt->check_perm(ctxt);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t\t      X86_ICPT_POST_EXCEPT);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\t\t\/* All REP prefixes have the same first termination condition *\/\n\t\t\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0) {\n\t\t\t\tctxt->eip = ctxt->_eip;\n\t\t\t\tctxt->eflags &= ~EFLG_RF;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((ctxt->src.type == OP_MEM) && !(ctxt->d & NoAccess)) {\n\t\trc = segmented_read(ctxt, ctxt->src.addr.mem,\n\t\t\t\t    ctxt->src.valptr, ctxt->src.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tctxt->src.orig_val64 = ctxt->src.val64;\n\t}\n\n\tif (ctxt->src2.type == OP_MEM) {\n\t\trc = segmented_read(ctxt, ctxt->src2.addr.mem,\n\t\t\t\t    &ctxt->src2.val, ctxt->src2.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif ((ctxt->d & DstMask) == ImplicitOps)\n\t\tgoto special_insn;\n\n\n\tif ((ctxt->dst.type == OP_MEM) && !(ctxt->d & Mov)) {\n\t\t\/* optimisation - avoid slow emulated read if Mov *\/\n\t\trc = segmented_read(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &ctxt->dst.val, ctxt->dst.bytes);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tctxt->dst.orig_val = ctxt->dst.val;\n\nspecial_insn:\n\n\tif (unlikely(ctxt->guest_mode) && (ctxt->d & Intercept)) {\n\t\trc = emulator_check_intercept(ctxt, ctxt->intercept,\n\t\t\t\t\t      X86_ICPT_POST_MEMACCESS);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\tif (ctxt->rep_prefix && (ctxt->d & String))\n\t\tctxt->eflags |= EFLG_RF;\n\telse\n\t\tctxt->eflags &= ~EFLG_RF;\n\n\tif (ctxt->execute) {\n\t\tif (ctxt->d & Fastop) {\n\t\t\tvoid (*fop)(struct fastop *) = (void *)ctxt->execute;\n\t\t\trc = fastop(ctxt, fop);\n\t\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\t\tgoto done;\n\t\t\tgoto writeback;\n\t\t}\n\t\trc = ctxt->execute(ctxt);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t\tgoto writeback;\n\t}\n\n\tif (ctxt->opcode_len == 2)\n\t\tgoto twobyte_insn;\n\telse if (ctxt->opcode_len == 3)\n\t\tgoto threebyte_insn;\n\n\tswitch (ctxt->b) {\n\tcase 0x63:\t\t\/* movsxd *\/\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tgoto cannot_emulate;\n\t\tctxt->dst.val = (s32) ctxt->src.val;\n\t\tbreak;\n\tcase 0x70 ... 0x7f: \/* jcc (short) *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x8d: \/* lea r16\/r32, m *\/\n\t\tctxt->dst.val = ctxt->src.addr.mem.ea;\n\t\tbreak;\n\tcase 0x90 ... 0x97: \/* nop \/ xchg reg, rax *\/\n\t\tif (ctxt->dst.addr.reg == reg_rmw(ctxt, VCPU_REGS_RAX))\n\t\t\tctxt->dst.type = OP_NONE;\n\t\telse\n\t\t\trc = em_xchg(ctxt);\n\t\tbreak;\n\tcase 0x98: \/* cbw\/cwde\/cdqe *\/\n\t\tswitch (ctxt->op_bytes) {\n\t\tcase 2: ctxt->dst.val = (s8)ctxt->dst.val; break;\n\t\tcase 4: ctxt->dst.val = (s16)ctxt->dst.val; break;\n\t\tcase 8: ctxt->dst.val = (s32)ctxt->dst.val; break;\n\t\t}\n\t\tbreak;\n\tcase 0xcc:\t\t\/* int3 *\/\n\t\trc = emulate_int(ctxt, 3);\n\t\tbreak;\n\tcase 0xcd:\t\t\/* int n *\/\n\t\trc = emulate_int(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0xce:\t\t\/* into *\/\n\t\tif (ctxt->eflags & EFLG_OF)\n\t\t\trc = emulate_int(ctxt, 4);\n\t\tbreak;\n\tcase 0xe9: \/* jmp rel *\/\n\tcase 0xeb: \/* jmp rel short *\/\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tctxt->dst.type = OP_NONE; \/* Disable writeback. *\/\n\t\tbreak;\n\tcase 0xf4:              \/* hlt *\/\n\t\tctxt->ops->halt(ctxt);\n\t\tbreak;\n\tcase 0xf5:\t\/* cmc *\/\n\t\t\/* complement carry flag from eflags reg *\/\n\t\tctxt->eflags ^= EFLG_CF;\n\t\tbreak;\n\tcase 0xf8: \/* clc *\/\n\t\tctxt->eflags &= ~EFLG_CF;\n\t\tbreak;\n\tcase 0xf9: \/* stc *\/\n\t\tctxt->eflags |= EFLG_CF;\n\t\tbreak;\n\tcase 0xfc: \/* cld *\/\n\t\tctxt->eflags &= ~EFLG_DF;\n\t\tbreak;\n\tcase 0xfd: \/* std *\/\n\t\tctxt->eflags |= EFLG_DF;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\nwriteback:\n\tif (ctxt->d & SrcWrite) {\n\t\tBUG_ON(ctxt->src.type == OP_MEM || ctxt->src.type == OP_MEM_STR);\n\t\trc = writeback(ctxt, &ctxt->src);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\tif (!(ctxt->d & NoWrite)) {\n\t\trc = writeback(ctxt, &ctxt->dst);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tgoto done;\n\t}\n\n\t\/*\n\t * restore dst type in case the decoding will be reused\n\t * (happens for string instruction )\n\t *\/\n\tctxt->dst.type = saved_dst_type;\n\n\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RSI, &ctxt->src);\n\n\tif ((ctxt->d & DstMask) == DstDI)\n\t\tstring_addr_inc(ctxt, VCPU_REGS_RDI, &ctxt->dst);\n\n\tif (ctxt->rep_prefix && (ctxt->d & String)) {\n\t\tunsigned int count;\n\t\tstruct read_cache *r = &ctxt->io_read;\n\t\tif ((ctxt->d & SrcMask) == SrcSI)\n\t\t\tcount = ctxt->src.count;\n\t\telse\n\t\t\tcount = ctxt->dst.count;\n\t\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX),\n\t\t\t\t-count);\n\n\t\tif (!string_insn_completed(ctxt)) {\n\t\t\t\/*\n\t\t\t * Re-enter guest when pio read ahead buffer is empty\n\t\t\t * or, if it is not used, after each 1024 iteration.\n\t\t\t *\/\n\t\t\tif ((r->end != 0 || reg_read(ctxt, VCPU_REGS_RCX) & 0x3ff) &&\n\t\t\t    (r->end == 0 || r->end != r->pos)) {\n\t\t\t\t\/*\n\t\t\t\t * Reset read cache. Usually happens before\n\t\t\t\t * decode, but since instruction is restarted\n\t\t\t\t * we have to do it here.\n\t\t\t\t *\/\n\t\t\t\tctxt->mem_read.end = 0;\n\t\t\t\twriteback_registers(ctxt);\n\t\t\t\treturn EMULATION_RESTART;\n\t\t\t}\n\t\t\tgoto done; \/* skip rip writeback *\/\n\t\t}\n\t\tctxt->eflags &= ~EFLG_RF;\n\t}\n\n\tctxt->eip = ctxt->_eip;\n\ndone:\n\tif (rc == X86EMUL_PROPAGATE_FAULT) {\n\t\tWARN_ON(ctxt->exception.vector > 0x1f);\n\t\tctxt->have_exception = true;\n\t}\n\tif (rc == X86EMUL_INTERCEPTED)\n\t\treturn EMULATION_INTERCEPTED;\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\twriteback_registers(ctxt);\n\n\treturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\n\ntwobyte_insn:\n\tswitch (ctxt->b) {\n\tcase 0x09:\t\t\/* wbinvd *\/\n\t\t(ctxt->ops->wbinvd)(ctxt);\n\t\tbreak;\n\tcase 0x08:\t\t\/* invd *\/\n\tcase 0x0d:\t\t\/* GrpP (prefetch) *\/\n\tcase 0x18:\t\t\/* Grp16 (prefetch\/nop) *\/\n\tcase 0x1f:\t\t\/* nop *\/\n\t\tbreak;\n\tcase 0x20: \/* mov cr, reg *\/\n\t\tctxt->dst.val = ops->get_cr(ctxt, ctxt->modrm_reg);\n\t\tbreak;\n\tcase 0x21: \/* mov from dr to reg *\/\n\t\tops->get_dr(ctxt, ctxt->modrm_reg, &ctxt->dst.val);\n\t\tbreak;\n\tcase 0x40 ... 0x4f:\t\/* cmov *\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\tctxt->dst.val = ctxt->src.val;\n\t\telse if (ctxt->mode != X86EMUL_MODE_PROT64 ||\n\t\t\t ctxt->op_bytes != 4)\n\t\t\tctxt->dst.type = OP_NONE; \/* no writeback *\/\n\t\tbreak;\n\tcase 0x80 ... 0x8f: \/* jnz rel, etc*\/\n\t\tif (test_cc(ctxt->b, ctxt->eflags))\n\t\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 0x90 ... 0x9f:     \/* setcc r\/m8 *\/\n\t\tctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);\n\t\tbreak;\n\tcase 0xae:              \/* clflush *\/\n\t\tbreak;\n\tcase 0xb6 ... 0xb7:\t\/* movzx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val\n\t\t\t\t\t\t       : (u16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xbe ... 0xbf:\t\/* movsx *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->src.bytes == 1) ? (s8) ctxt->src.val :\n\t\t\t\t\t\t\t(s16) ctxt->src.val;\n\t\tbreak;\n\tcase 0xc3:\t\t\/* movnti *\/\n\t\tctxt->dst.bytes = ctxt->op_bytes;\n\t\tctxt->dst.val = (ctxt->op_bytes == 8) ? (u64) ctxt->src.val :\n\t\t\t\t\t\t\t(u32) ctxt->src.val;\n\t\tbreak;\n\tdefault:\n\t\tgoto cannot_emulate;\n\t}\n\nthreebyte_insn:\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tgoto writeback;\n\ncannot_emulate:\n\treturn EMULATION_FAILED;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int assign_eip_far(struct x86_emulate_ctxt *ctxt, ulong dst,\n\t\t\t       int cs_l)\n{\n\tswitch (ctxt->op_bytes) {\n\tcase 2:\n\t\tctxt->_eip = (u16)dst;\n\t\tbreak;\n\tcase 4:\n\t\tctxt->_eip = (u32)dst;\n\t\tbreak;\n\tcase 8:\n\t\tif ((cs_l && is_noncanonical_address(dst)) ||\n\t\t    (!cs_l && (dst & ~(u32)-1)))\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tctxt->_eip = dst;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unsupported eip assignment size\\n\");\n\t}\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int assign_eip_far(struct x86_emulate_ctxt *ctxt, ulong dst,\n\t\t\t       int cs_l)\n{\n\tswitch (ctxt->op_bytes) {\n\tcase 2:\n\t\tctxt->_eip = (u16)dst;\n\t\tbreak;\n\tcase 4:\n\t\tctxt->_eip = (u32)dst;\n\t\tbreak;\n\tcase 8:\n\t\tif ((cs_l && is_noncanonical_address(dst)) ||\n\t\t    (!cs_l && (dst & ~(u32)-1)))\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tctxt->_eip = dst;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unsupported eip assignment size\\n\");\n\t}\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)\n{\n\treturn assign_eip_far(ctxt, dst, ctxt->mode == X86EMUL_MODE_PROT64);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)\n{\n\treturn assign_eip_far(ctxt, dst, ctxt->mode == X86EMUL_MODE_PROT64);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)\n{\n\treturn assign_eip_near(ctxt, ctxt->_eip + rel);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline int jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)\n{\n\treturn assign_eip_near(ctxt, ctxt->_eip + rel);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline void assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)\n{\n\tswitch (ctxt->op_bytes) {\n\tcase 2:\n\t\tctxt->_eip = (u16)dst;\n\t\tbreak;\n\tcase 4:\n\t\tctxt->_eip = (u32)dst;\n\t\tbreak;\n\tcase 8:\n\t\tctxt->_eip = dst;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unsupported eip assignment size\\n\");\n\t}\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline void assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)\n{\n\tswitch (ctxt->op_bytes) {\n\tcase 2:\n\t\tctxt->_eip = (u16)dst;\n\t\tbreak;\n\tcase 4:\n\t\tctxt->_eip = (u32)dst;\n\t\tbreak;\n\tcase 8:\n\t\tctxt->_eip = dst;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unsupported eip assignment size\\n\");\n\t}\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline void jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)\n{\n\tassign_eip_near(ctxt, ctxt->_eip + rel);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static inline void jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)\n{\n\tassign_eip_near(ctxt, ctxt->_eip + rel);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tlong rel = ctxt->src.val;\n\n\tctxt->src.val = (unsigned long)ctxt->_eip;\n\trc = jmp_rel(ctxt, rel);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\treturn em_push(ctxt);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tlong rel = ctxt->src.val;\n\n\tctxt->src.val = (unsigned long)ctxt->_eip;\n\trc = jmp_rel(ctxt, rel);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\treturn em_push(ctxt);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call(struct x86_emulate_ctxt *ctxt)\n{\n\tlong rel = ctxt->src.val;\n\n\tctxt->src.val = (unsigned long)ctxt->_eip;\n\tjmp_rel(ctxt, rel);\n\treturn em_push(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call(struct x86_emulate_ctxt *ctxt)\n{\n\tlong rel = ctxt->src.val;\n\n\tctxt->src.val = (unsigned long)ctxt->_eip;\n\tjmp_rel(ctxt, rel);\n\treturn em_push(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_grp45(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tswitch (ctxt->modrm_reg) {\n\tcase 2: \/* call near abs *\/ {\n\t\tlong int old_eip;\n\t\told_eip = ctxt->_eip;\n\t\tctxt->_eip = ctxt->src.val;\n\t\tctxt->src.val = old_eip;\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\tcase 4: \/* jmp abs *\/\n\t\tctxt->_eip = ctxt->src.val;\n\t\tbreak;\n\tcase 5: \/* jmp far *\/\n\t\trc = em_jmp_far(ctxt);\n\t\tbreak;\n\tcase 6:\t\/* push *\/\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_grp45(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tswitch (ctxt->modrm_reg) {\n\tcase 2: \/* call near abs *\/ {\n\t\tlong int old_eip;\n\t\told_eip = ctxt->_eip;\n\t\tctxt->_eip = ctxt->src.val;\n\t\tctxt->src.val = old_eip;\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\tcase 4: \/* jmp abs *\/\n\t\tctxt->_eip = ctxt->src.val;\n\t\tbreak;\n\tcase 5: \/* jmp far *\/\n\t\trc = em_jmp_far(ctxt);\n\t\tbreak;\n\tcase 6:\t\/* push *\/\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_grp45(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tswitch (ctxt->modrm_reg) {\n\tcase 2: \/* call near abs *\/ {\n\t\tlong int old_eip;\n\t\told_eip = ctxt->_eip;\n\t\trc = assign_eip_near(ctxt, ctxt->src.val);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tbreak;\n\t\tctxt->src.val = old_eip;\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\tcase 4: \/* jmp abs *\/\n\t\trc = assign_eip_near(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 5: \/* jmp far *\/\n\t\trc = em_jmp_far(ctxt);\n\t\tbreak;\n\tcase 6:\t\/* push *\/\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_grp45(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tswitch (ctxt->modrm_reg) {\n\tcase 2: \/* call near abs *\/ {\n\t\tlong int old_eip;\n\t\told_eip = ctxt->_eip;\n\t\trc = assign_eip_near(ctxt, ctxt->src.val);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\tbreak;\n\t\tctxt->src.val = old_eip;\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\tcase 4: \/* jmp abs *\/\n\t\trc = assign_eip_near(ctxt, ctxt->src.val);\n\t\tbreak;\n\tcase 5: \/* jmp far *\/\n\t\trc = em_jmp_far(ctxt);\n\t\tbreak;\n\tcase 6:\t\/* push *\/\n\t\trc = em_push(ctxt);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jcxz(struct x86_emulate_ctxt *ctxt)\n{\n\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jcxz(struct x86_emulate_ctxt *ctxt)\n{\n\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jcxz(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jcxz(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_loop(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);\n\tif ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&\n\t    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_loop(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc = X86EMUL_CONTINUE;\n\n\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);\n\tif ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&\n\t    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))\n\t\trc = jmp_rel(ctxt, ctxt->src.val);\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_loop(struct x86_emulate_ctxt *ctxt)\n{\n\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);\n\tif ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&\n\t    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_loop(struct x86_emulate_ctxt *ctxt)\n{\n\tregister_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);\n\tif ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&\n\t    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))\n\t\tjmp_rel(ctxt, ctxt->src.val);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret(struct x86_emulate_ctxt *ctxt)\n{\n\tctxt->dst.type = OP_REG;\n\tctxt->dst.addr.reg = &ctxt->_eip;\n\tctxt->dst.bytes = ctxt->op_bytes;\n\treturn em_pop(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret(struct x86_emulate_ctxt *ctxt)\n{\n\tctxt->dst.type = OP_REG;\n\tctxt->dst.addr.reg = &ctxt->_eip;\n\tctxt->dst.bytes = ctxt->op_bytes;\n\treturn em_pop(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip;\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\treturn assign_eip_near(ctxt, eip);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip;\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\treturn assign_eip_near(ctxt, eip);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip;\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_near(ctxt, eip);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trsp_increment(ctxt, ctxt->src.val);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip;\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_near(ctxt, eip);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trsp_increment(ctxt, ctxt->src.val);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\n\tctxt->dst.type = OP_REG;\n\tctxt->dst.addr.reg = &ctxt->_eip;\n\tctxt->dst.bytes = ctxt->op_bytes;\n\trc = emulate_pop(ctxt, &ctxt->dst.val, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trsp_increment(ctxt, ctxt->src.val);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\n\tctxt->dst.type = OP_REG;\n\tctxt->dst.addr.reg = &ctxt->_eip;\n\tctxt->dst.bytes = ctxt->op_bytes;\n\trc = emulate_pop(ctxt, &ctxt->dst.val, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trsp_increment(ctxt, ctxt->src.val);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_sysexit(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data, rcx, rdx;\n\tint usermode;\n\tu16 cs_sel = 0, ss_sel = 0;\n\n\t\/* inject #GP if in real mode or Virtual 8086 mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif ((ctxt->rex_prefix & 0x8) != 0x0)\n\t\tusermode = X86EMUL_MODE_PROT64;\n\telse\n\t\tusermode = X86EMUL_MODE_PROT32;\n\n\trcx = reg_read(ctxt, VCPU_REGS_RCX);\n\trdx = reg_read(ctxt, VCPU_REGS_RDX);\n\n\tcs.dpl = 3;\n\tss.dpl = 3;\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (usermode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tcs_sel = (u16)(msr_data + 16);\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = (u16)(msr_data + 24);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tcs_sel = (u16)(msr_data + 32);\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = cs_sel + 8;\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t\tif (is_noncanonical_address(rcx) ||\n\t\t    is_noncanonical_address(rdx))\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\t}\n\tcs_sel |= SELECTOR_RPL_MASK;\n\tss_sel |= SELECTOR_RPL_MASK;\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->_eip = rdx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = rcx;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_sysexit(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data, rcx, rdx;\n\tint usermode;\n\tu16 cs_sel = 0, ss_sel = 0;\n\n\t\/* inject #GP if in real mode or Virtual 8086 mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif ((ctxt->rex_prefix & 0x8) != 0x0)\n\t\tusermode = X86EMUL_MODE_PROT64;\n\telse\n\t\tusermode = X86EMUL_MODE_PROT32;\n\n\trcx = reg_read(ctxt, VCPU_REGS_RCX);\n\trdx = reg_read(ctxt, VCPU_REGS_RDX);\n\n\tcs.dpl = 3;\n\tss.dpl = 3;\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (usermode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tcs_sel = (u16)(msr_data + 16);\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = (u16)(msr_data + 24);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tcs_sel = (u16)(msr_data + 32);\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = cs_sel + 8;\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t\tif (is_noncanonical_address(rcx) ||\n\t\t    is_noncanonical_address(rdx))\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\t}\n\tcs_sel |= SELECTOR_RPL_MASK;\n\tss_sel |= SELECTOR_RPL_MASK;\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->_eip = rdx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = rcx;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_sysexit(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tint usermode;\n\tu16 cs_sel = 0, ss_sel = 0;\n\n\t\/* inject #GP if in real mode or Virtual 8086 mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif ((ctxt->rex_prefix & 0x8) != 0x0)\n\t\tusermode = X86EMUL_MODE_PROT64;\n\telse\n\t\tusermode = X86EMUL_MODE_PROT32;\n\n\tcs.dpl = 3;\n\tss.dpl = 3;\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (usermode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tcs_sel = (u16)(msr_data + 16);\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = (u16)(msr_data + 24);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tcs_sel = (u16)(msr_data + 32);\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = cs_sel + 8;\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t\tbreak;\n\t}\n\tcs_sel |= SELECTOR_RPL_MASK;\n\tss_sel |= SELECTOR_RPL_MASK;\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->_eip = reg_read(ctxt, VCPU_REGS_RDX);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = reg_read(ctxt, VCPU_REGS_RCX);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_sysexit(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tint usermode;\n\tu16 cs_sel = 0, ss_sel = 0;\n\n\t\/* inject #GP if in real mode or Virtual 8086 mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL ||\n\t    ctxt->mode == X86EMUL_MODE_VM86)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tif ((ctxt->rex_prefix & 0x8) != 0x0)\n\t\tusermode = X86EMUL_MODE_PROT64;\n\telse\n\t\tusermode = X86EMUL_MODE_PROT32;\n\n\tcs.dpl = 3;\n\tss.dpl = 3;\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (usermode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tcs_sel = (u16)(msr_data + 16);\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = (u16)(msr_data + 24);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tcs_sel = (u16)(msr_data + 32);\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tss_sel = cs_sel + 8;\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t\tbreak;\n\t}\n\tcs_sel |= SELECTOR_RPL_MASK;\n\tss_sel |= SELECTOR_RPL_MASK;\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tctxt->_eip = reg_read(ctxt, VCPU_REGS_RDX);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = reg_read(ctxt, VCPU_REGS_RCX);\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     u16 selector, int seg, u8 cpl,\n\t\t\t\t     bool in_task_switch,\n\t\t\t\t     struct desc_struct *desc)\n{\n\tstruct desc_struct seg_desc, old_desc;\n\tu8 dpl, rpl;\n\tunsigned err_vec = GP_VECTOR;\n\tu32 err_code = 0;\n\tbool null_selector = !(selector & ~0x3); \/* 0000-0003 are null *\/\n\tulong desc_addr;\n\tint ret;\n\tu16 dummy;\n\tu32 base3 = 0;\n\n\tmemset(&seg_desc, 0, sizeof seg_desc);\n\n\tif (ctxt->mode == X86EMUL_MODE_REAL) {\n\t\t\/* set real mode segment descriptor (keep limit etc. for\n\t\t * unreal mode) *\/\n\t\tctxt->ops->get_segment(ctxt, &dummy, &seg_desc, NULL, seg);\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tgoto load;\n\t} else if (seg <= VCPU_SREG_GS && ctxt->mode == X86EMUL_MODE_VM86) {\n\t\t\/* VM86 needs a clean new segment descriptor *\/\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tset_desc_limit(&seg_desc, 0xffff);\n\t\tseg_desc.type = 3;\n\t\tseg_desc.p = 1;\n\t\tseg_desc.s = 1;\n\t\tseg_desc.dpl = 3;\n\t\tgoto load;\n\t}\n\n\trpl = selector & 3;\n\n\t\/* NULL selector is not valid for TR, CS and SS (except for long mode) *\/\n\tif ((seg == VCPU_SREG_CS\n\t     || (seg == VCPU_SREG_SS\n\t\t && (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))\n\t     || seg == VCPU_SREG_TR)\n\t    && null_selector)\n\t\tgoto exception;\n\n\t\/* TR should be in GDT only *\/\n\tif (seg == VCPU_SREG_TR && (selector & (1 << 2)))\n\t\tgoto exception;\n\n\tif (null_selector) \/* for NULL selector skip all following checks *\/\n\t\tgoto load;\n\n\tret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\terr_code = selector & 0xfffc;\n\terr_vec = in_task_switch ? TS_VECTOR : GP_VECTOR;\n\n\t\/* can't load system descriptor into segment selector *\/\n\tif (seg <= VCPU_SREG_GS && !seg_desc.s)\n\t\tgoto exception;\n\n\tif (!seg_desc.p) {\n\t\terr_vec = (seg == VCPU_SREG_SS) ? SS_VECTOR : NP_VECTOR;\n\t\tgoto exception;\n\t}\n\n\tdpl = seg_desc.dpl;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_SS:\n\t\t\/*\n\t\t * segment is not a writable data segment or segment\n\t\t * selector's RPL != CPL or segment selector's RPL != CPL\n\t\t *\/\n\t\tif (rpl != cpl || (seg_desc.type & 0xa) != 0x2 || dpl != cpl)\n\t\t\tgoto exception;\n\t\tbreak;\n\tcase VCPU_SREG_CS:\n\t\tif (!(seg_desc.type & 8))\n\t\t\tgoto exception;\n\n\t\tif (seg_desc.type & 4) {\n\t\t\t\/* conforming *\/\n\t\t\tif (dpl > cpl)\n\t\t\t\tgoto exception;\n\t\t} else {\n\t\t\t\/* nonconforming *\/\n\t\t\tif (rpl > cpl || dpl != cpl)\n\t\t\t\tgoto exception;\n\t\t}\n\t\t\/* in long-mode d\/b must be clear if l is set *\/\n\t\tif (seg_desc.d && seg_desc.l) {\n\t\t\tu64 efer = 0;\n\n\t\t\tctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\n\t\t\tif (efer & EFER_LMA)\n\t\t\t\tgoto exception;\n\t\t}\n\n\t\t\/* CS(RPL) <- CPL *\/\n\t\tselector = (selector & 0xfffc) | cpl;\n\t\tbreak;\n\tcase VCPU_SREG_TR:\n\t\tif (seg_desc.s || (seg_desc.type != 1 && seg_desc.type != 9))\n\t\t\tgoto exception;\n\t\told_desc = seg_desc;\n\t\tseg_desc.type |= 2; \/* busy *\/\n\t\tret = ctxt->ops->cmpxchg_emulated(ctxt, desc_addr, &old_desc, &seg_desc,\n\t\t\t\t\t\t  sizeof(seg_desc), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase VCPU_SREG_LDTR:\n\t\tif (seg_desc.s || seg_desc.type != 2)\n\t\t\tgoto exception;\n\t\tbreak;\n\tdefault: \/*  DS, ES, FS, or GS *\/\n\t\t\/*\n\t\t * segment is not a data or readable code segment or\n\t\t * ((segment is a data or nonconforming code segment)\n\t\t * and (both RPL and CPL > DPL))\n\t\t *\/\n\t\tif ((seg_desc.type & 0xa) == 0x8 ||\n\t\t    (((seg_desc.type & 0xc) != 0xc) &&\n\t\t     (rpl > dpl && cpl > dpl)))\n\t\t\tgoto exception;\n\t\tbreak;\n\t}\n\n\tif (seg_desc.s) {\n\t\t\/* mark segment as accessed *\/\n\t\tseg_desc.type |= 1;\n\t\tret = write_segment_descriptor(ctxt, selector, &seg_desc);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t} else if (ctxt->mode == X86EMUL_MODE_PROT64) {\n\t\tret = ctxt->ops->read_std(ctxt, desc_addr+8, &base3,\n\t\t\t\tsizeof(base3), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t}\nload:\n\tctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);\n\tif (desc)\n\t\t*desc = seg_desc;\n\treturn X86EMUL_CONTINUE;\nexception:\n\treturn emulate_exception(ctxt, err_vec, err_code, true);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     u16 selector, int seg, u8 cpl,\n\t\t\t\t     bool in_task_switch,\n\t\t\t\t     struct desc_struct *desc)\n{\n\tstruct desc_struct seg_desc, old_desc;\n\tu8 dpl, rpl;\n\tunsigned err_vec = GP_VECTOR;\n\tu32 err_code = 0;\n\tbool null_selector = !(selector & ~0x3); \/* 0000-0003 are null *\/\n\tulong desc_addr;\n\tint ret;\n\tu16 dummy;\n\tu32 base3 = 0;\n\n\tmemset(&seg_desc, 0, sizeof seg_desc);\n\n\tif (ctxt->mode == X86EMUL_MODE_REAL) {\n\t\t\/* set real mode segment descriptor (keep limit etc. for\n\t\t * unreal mode) *\/\n\t\tctxt->ops->get_segment(ctxt, &dummy, &seg_desc, NULL, seg);\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tgoto load;\n\t} else if (seg <= VCPU_SREG_GS && ctxt->mode == X86EMUL_MODE_VM86) {\n\t\t\/* VM86 needs a clean new segment descriptor *\/\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tset_desc_limit(&seg_desc, 0xffff);\n\t\tseg_desc.type = 3;\n\t\tseg_desc.p = 1;\n\t\tseg_desc.s = 1;\n\t\tseg_desc.dpl = 3;\n\t\tgoto load;\n\t}\n\n\trpl = selector & 3;\n\n\t\/* NULL selector is not valid for TR, CS and SS (except for long mode) *\/\n\tif ((seg == VCPU_SREG_CS\n\t     || (seg == VCPU_SREG_SS\n\t\t && (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))\n\t     || seg == VCPU_SREG_TR)\n\t    && null_selector)\n\t\tgoto exception;\n\n\t\/* TR should be in GDT only *\/\n\tif (seg == VCPU_SREG_TR && (selector & (1 << 2)))\n\t\tgoto exception;\n\n\tif (null_selector) \/* for NULL selector skip all following checks *\/\n\t\tgoto load;\n\n\tret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\terr_code = selector & 0xfffc;\n\terr_vec = in_task_switch ? TS_VECTOR : GP_VECTOR;\n\n\t\/* can't load system descriptor into segment selector *\/\n\tif (seg <= VCPU_SREG_GS && !seg_desc.s)\n\t\tgoto exception;\n\n\tif (!seg_desc.p) {\n\t\terr_vec = (seg == VCPU_SREG_SS) ? SS_VECTOR : NP_VECTOR;\n\t\tgoto exception;\n\t}\n\n\tdpl = seg_desc.dpl;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_SS:\n\t\t\/*\n\t\t * segment is not a writable data segment or segment\n\t\t * selector's RPL != CPL or segment selector's RPL != CPL\n\t\t *\/\n\t\tif (rpl != cpl || (seg_desc.type & 0xa) != 0x2 || dpl != cpl)\n\t\t\tgoto exception;\n\t\tbreak;\n\tcase VCPU_SREG_CS:\n\t\tif (!(seg_desc.type & 8))\n\t\t\tgoto exception;\n\n\t\tif (seg_desc.type & 4) {\n\t\t\t\/* conforming *\/\n\t\t\tif (dpl > cpl)\n\t\t\t\tgoto exception;\n\t\t} else {\n\t\t\t\/* nonconforming *\/\n\t\t\tif (rpl > cpl || dpl != cpl)\n\t\t\t\tgoto exception;\n\t\t}\n\t\t\/* in long-mode d\/b must be clear if l is set *\/\n\t\tif (seg_desc.d && seg_desc.l) {\n\t\t\tu64 efer = 0;\n\n\t\t\tctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\n\t\t\tif (efer & EFER_LMA)\n\t\t\t\tgoto exception;\n\t\t}\n\n\t\t\/* CS(RPL) <- CPL *\/\n\t\tselector = (selector & 0xfffc) | cpl;\n\t\tbreak;\n\tcase VCPU_SREG_TR:\n\t\tif (seg_desc.s || (seg_desc.type != 1 && seg_desc.type != 9))\n\t\t\tgoto exception;\n\t\told_desc = seg_desc;\n\t\tseg_desc.type |= 2; \/* busy *\/\n\t\tret = ctxt->ops->cmpxchg_emulated(ctxt, desc_addr, &old_desc, &seg_desc,\n\t\t\t\t\t\t  sizeof(seg_desc), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase VCPU_SREG_LDTR:\n\t\tif (seg_desc.s || seg_desc.type != 2)\n\t\t\tgoto exception;\n\t\tbreak;\n\tdefault: \/*  DS, ES, FS, or GS *\/\n\t\t\/*\n\t\t * segment is not a data or readable code segment or\n\t\t * ((segment is a data or nonconforming code segment)\n\t\t * and (both RPL and CPL > DPL))\n\t\t *\/\n\t\tif ((seg_desc.type & 0xa) == 0x8 ||\n\t\t    (((seg_desc.type & 0xc) != 0xc) &&\n\t\t     (rpl > dpl && cpl > dpl)))\n\t\t\tgoto exception;\n\t\tbreak;\n\t}\n\n\tif (seg_desc.s) {\n\t\t\/* mark segment as accessed *\/\n\t\tseg_desc.type |= 1;\n\t\tret = write_segment_descriptor(ctxt, selector, &seg_desc);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t} else if (ctxt->mode == X86EMUL_MODE_PROT64) {\n\t\tret = ctxt->ops->read_std(ctxt, desc_addr+8, &base3,\n\t\t\t\tsizeof(base3), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t}\nload:\n\tctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);\n\tif (desc)\n\t\t*desc = seg_desc;\n\treturn X86EMUL_CONTINUE;\nexception:\n\treturn emulate_exception(ctxt, err_vec, err_code, true);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     u16 selector, int seg, u8 cpl, bool in_task_switch)\n{\n\tstruct desc_struct seg_desc, old_desc;\n\tu8 dpl, rpl;\n\tunsigned err_vec = GP_VECTOR;\n\tu32 err_code = 0;\n\tbool null_selector = !(selector & ~0x3); \/* 0000-0003 are null *\/\n\tulong desc_addr;\n\tint ret;\n\tu16 dummy;\n\tu32 base3 = 0;\n\n\tmemset(&seg_desc, 0, sizeof seg_desc);\n\n\tif (ctxt->mode == X86EMUL_MODE_REAL) {\n\t\t\/* set real mode segment descriptor (keep limit etc. for\n\t\t * unreal mode) *\/\n\t\tctxt->ops->get_segment(ctxt, &dummy, &seg_desc, NULL, seg);\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tgoto load;\n\t} else if (seg <= VCPU_SREG_GS && ctxt->mode == X86EMUL_MODE_VM86) {\n\t\t\/* VM86 needs a clean new segment descriptor *\/\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tset_desc_limit(&seg_desc, 0xffff);\n\t\tseg_desc.type = 3;\n\t\tseg_desc.p = 1;\n\t\tseg_desc.s = 1;\n\t\tseg_desc.dpl = 3;\n\t\tgoto load;\n\t}\n\n\trpl = selector & 3;\n\n\t\/* NULL selector is not valid for TR, CS and SS (except for long mode) *\/\n\tif ((seg == VCPU_SREG_CS\n\t     || (seg == VCPU_SREG_SS\n\t\t && (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))\n\t     || seg == VCPU_SREG_TR)\n\t    && null_selector)\n\t\tgoto exception;\n\n\t\/* TR should be in GDT only *\/\n\tif (seg == VCPU_SREG_TR && (selector & (1 << 2)))\n\t\tgoto exception;\n\n\tif (null_selector) \/* for NULL selector skip all following checks *\/\n\t\tgoto load;\n\n\tret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\terr_code = selector & 0xfffc;\n\terr_vec = in_task_switch ? TS_VECTOR : GP_VECTOR;\n\n\t\/* can't load system descriptor into segment selector *\/\n\tif (seg <= VCPU_SREG_GS && !seg_desc.s)\n\t\tgoto exception;\n\n\tif (!seg_desc.p) {\n\t\terr_vec = (seg == VCPU_SREG_SS) ? SS_VECTOR : NP_VECTOR;\n\t\tgoto exception;\n\t}\n\n\tdpl = seg_desc.dpl;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_SS:\n\t\t\/*\n\t\t * segment is not a writable data segment or segment\n\t\t * selector's RPL != CPL or segment selector's RPL != CPL\n\t\t *\/\n\t\tif (rpl != cpl || (seg_desc.type & 0xa) != 0x2 || dpl != cpl)\n\t\t\tgoto exception;\n\t\tbreak;\n\tcase VCPU_SREG_CS:\n\t\tif (!(seg_desc.type & 8))\n\t\t\tgoto exception;\n\n\t\tif (seg_desc.type & 4) {\n\t\t\t\/* conforming *\/\n\t\t\tif (dpl > cpl)\n\t\t\t\tgoto exception;\n\t\t} else {\n\t\t\t\/* nonconforming *\/\n\t\t\tif (rpl > cpl || dpl != cpl)\n\t\t\t\tgoto exception;\n\t\t}\n\t\t\/* in long-mode d\/b must be clear if l is set *\/\n\t\tif (seg_desc.d && seg_desc.l) {\n\t\t\tu64 efer = 0;\n\n\t\t\tctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\n\t\t\tif (efer & EFER_LMA)\n\t\t\t\tgoto exception;\n\t\t}\n\n\t\t\/* CS(RPL) <- CPL *\/\n\t\tselector = (selector & 0xfffc) | cpl;\n\t\tbreak;\n\tcase VCPU_SREG_TR:\n\t\tif (seg_desc.s || (seg_desc.type != 1 && seg_desc.type != 9))\n\t\t\tgoto exception;\n\t\told_desc = seg_desc;\n\t\tseg_desc.type |= 2; \/* busy *\/\n\t\tret = ctxt->ops->cmpxchg_emulated(ctxt, desc_addr, &old_desc, &seg_desc,\n\t\t\t\t\t\t  sizeof(seg_desc), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase VCPU_SREG_LDTR:\n\t\tif (seg_desc.s || seg_desc.type != 2)\n\t\t\tgoto exception;\n\t\tbreak;\n\tdefault: \/*  DS, ES, FS, or GS *\/\n\t\t\/*\n\t\t * segment is not a data or readable code segment or\n\t\t * ((segment is a data or nonconforming code segment)\n\t\t * and (both RPL and CPL > DPL))\n\t\t *\/\n\t\tif ((seg_desc.type & 0xa) == 0x8 ||\n\t\t    (((seg_desc.type & 0xc) != 0xc) &&\n\t\t     (rpl > dpl && cpl > dpl)))\n\t\t\tgoto exception;\n\t\tbreak;\n\t}\n\n\tif (seg_desc.s) {\n\t\t\/* mark segment as accessed *\/\n\t\tseg_desc.type |= 1;\n\t\tret = write_segment_descriptor(ctxt, selector, &seg_desc);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t} else if (ctxt->mode == X86EMUL_MODE_PROT64) {\n\t\tret = ctxt->ops->read_std(ctxt, desc_addr+8, &base3,\n\t\t\t\tsizeof(base3), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t}\nload:\n\tctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);\n\treturn X86EMUL_CONTINUE;\nexception:\n\treturn emulate_exception(ctxt, err_vec, err_code, true);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     u16 selector, int seg, u8 cpl, bool in_task_switch)\n{\n\tstruct desc_struct seg_desc, old_desc;\n\tu8 dpl, rpl;\n\tunsigned err_vec = GP_VECTOR;\n\tu32 err_code = 0;\n\tbool null_selector = !(selector & ~0x3); \/* 0000-0003 are null *\/\n\tulong desc_addr;\n\tint ret;\n\tu16 dummy;\n\tu32 base3 = 0;\n\n\tmemset(&seg_desc, 0, sizeof seg_desc);\n\n\tif (ctxt->mode == X86EMUL_MODE_REAL) {\n\t\t\/* set real mode segment descriptor (keep limit etc. for\n\t\t * unreal mode) *\/\n\t\tctxt->ops->get_segment(ctxt, &dummy, &seg_desc, NULL, seg);\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tgoto load;\n\t} else if (seg <= VCPU_SREG_GS && ctxt->mode == X86EMUL_MODE_VM86) {\n\t\t\/* VM86 needs a clean new segment descriptor *\/\n\t\tset_desc_base(&seg_desc, selector << 4);\n\t\tset_desc_limit(&seg_desc, 0xffff);\n\t\tseg_desc.type = 3;\n\t\tseg_desc.p = 1;\n\t\tseg_desc.s = 1;\n\t\tseg_desc.dpl = 3;\n\t\tgoto load;\n\t}\n\n\trpl = selector & 3;\n\n\t\/* NULL selector is not valid for TR, CS and SS (except for long mode) *\/\n\tif ((seg == VCPU_SREG_CS\n\t     || (seg == VCPU_SREG_SS\n\t\t && (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))\n\t     || seg == VCPU_SREG_TR)\n\t    && null_selector)\n\t\tgoto exception;\n\n\t\/* TR should be in GDT only *\/\n\tif (seg == VCPU_SREG_TR && (selector & (1 << 2)))\n\t\tgoto exception;\n\n\tif (null_selector) \/* for NULL selector skip all following checks *\/\n\t\tgoto load;\n\n\tret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\terr_code = selector & 0xfffc;\n\terr_vec = in_task_switch ? TS_VECTOR : GP_VECTOR;\n\n\t\/* can't load system descriptor into segment selector *\/\n\tif (seg <= VCPU_SREG_GS && !seg_desc.s)\n\t\tgoto exception;\n\n\tif (!seg_desc.p) {\n\t\terr_vec = (seg == VCPU_SREG_SS) ? SS_VECTOR : NP_VECTOR;\n\t\tgoto exception;\n\t}\n\n\tdpl = seg_desc.dpl;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_SS:\n\t\t\/*\n\t\t * segment is not a writable data segment or segment\n\t\t * selector's RPL != CPL or segment selector's RPL != CPL\n\t\t *\/\n\t\tif (rpl != cpl || (seg_desc.type & 0xa) != 0x2 || dpl != cpl)\n\t\t\tgoto exception;\n\t\tbreak;\n\tcase VCPU_SREG_CS:\n\t\tif (!(seg_desc.type & 8))\n\t\t\tgoto exception;\n\n\t\tif (seg_desc.type & 4) {\n\t\t\t\/* conforming *\/\n\t\t\tif (dpl > cpl)\n\t\t\t\tgoto exception;\n\t\t} else {\n\t\t\t\/* nonconforming *\/\n\t\t\tif (rpl > cpl || dpl != cpl)\n\t\t\t\tgoto exception;\n\t\t}\n\t\t\/* in long-mode d\/b must be clear if l is set *\/\n\t\tif (seg_desc.d && seg_desc.l) {\n\t\t\tu64 efer = 0;\n\n\t\t\tctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\n\t\t\tif (efer & EFER_LMA)\n\t\t\t\tgoto exception;\n\t\t}\n\n\t\t\/* CS(RPL) <- CPL *\/\n\t\tselector = (selector & 0xfffc) | cpl;\n\t\tbreak;\n\tcase VCPU_SREG_TR:\n\t\tif (seg_desc.s || (seg_desc.type != 1 && seg_desc.type != 9))\n\t\t\tgoto exception;\n\t\told_desc = seg_desc;\n\t\tseg_desc.type |= 2; \/* busy *\/\n\t\tret = ctxt->ops->cmpxchg_emulated(ctxt, desc_addr, &old_desc, &seg_desc,\n\t\t\t\t\t\t  sizeof(seg_desc), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase VCPU_SREG_LDTR:\n\t\tif (seg_desc.s || seg_desc.type != 2)\n\t\t\tgoto exception;\n\t\tbreak;\n\tdefault: \/*  DS, ES, FS, or GS *\/\n\t\t\/*\n\t\t * segment is not a data or readable code segment or\n\t\t * ((segment is a data or nonconforming code segment)\n\t\t * and (both RPL and CPL > DPL))\n\t\t *\/\n\t\tif ((seg_desc.type & 0xa) == 0x8 ||\n\t\t    (((seg_desc.type & 0xc) != 0xc) &&\n\t\t     (rpl > dpl && cpl > dpl)))\n\t\t\tgoto exception;\n\t\tbreak;\n\t}\n\n\tif (seg_desc.s) {\n\t\t\/* mark segment as accessed *\/\n\t\tseg_desc.type |= 1;\n\t\tret = write_segment_descriptor(ctxt, selector, &seg_desc);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t} else if (ctxt->mode == X86EMUL_MODE_PROT64) {\n\t\tret = ctxt->ops->read_std(ctxt, desc_addr+8, &base3,\n\t\t\t\tsizeof(base3), &ctxt->exception);\n\t\tif (ret != X86EMUL_CONTINUE)\n\t\t\treturn ret;\n\t}\nload:\n\tctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);\n\treturn X86EMUL_CONTINUE;\nexception:\n\treturn emulate_exception(ctxt, err_vec, err_code, true);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call_far(struct x86_emulate_ctxt *ctxt)\n{\n\tu16 sel, old_cs;\n\tulong old_eip;\n\tint rc;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\n\told_eip = ctxt->_eip;\n\tops->get_segment(ctxt, &old_cs, &old_desc, NULL, VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_CONTINUE;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\n\tctxt->src.val = old_cs;\n\trc = em_push(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\n\tctxt->src.val = old_eip;\n\trc = em_push(ctxt);\n\t\/* If we failed, we tainted the memory, but the very least we should\n\t   restore cs *\/\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\treturn rc;\nfail:\n\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n\treturn rc;\n\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call_far(struct x86_emulate_ctxt *ctxt)\n{\n\tu16 sel, old_cs;\n\tulong old_eip;\n\tint rc;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\n\told_eip = ctxt->_eip;\n\tops->get_segment(ctxt, &old_cs, &old_desc, NULL, VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_CONTINUE;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\n\tctxt->src.val = old_cs;\n\trc = em_push(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\n\tctxt->src.val = old_eip;\n\trc = em_push(ctxt);\n\t\/* If we failed, we tainted the memory, but the very least we should\n\t   restore cs *\/\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto fail;\n\treturn rc;\nfail:\n\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n\treturn rc;\n\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call_far(struct x86_emulate_ctxt *ctxt)\n{\n\tu16 sel, old_cs;\n\tulong old_eip;\n\tint rc;\n\n\told_cs = get_segment_selector(ctxt, VCPU_SREG_CS);\n\told_eip = ctxt->_eip;\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\tif (load_segment_descriptor(ctxt, sel, VCPU_SREG_CS))\n\t\treturn X86EMUL_CONTINUE;\n\n\tctxt->_eip = 0;\n\tmemcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);\n\n\tctxt->src.val = old_cs;\n\trc = em_push(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->src.val = old_eip;\n\treturn em_push(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_call_far(struct x86_emulate_ctxt *ctxt)\n{\n\tu16 sel, old_cs;\n\tulong old_eip;\n\tint rc;\n\n\told_cs = get_segment_selector(ctxt, VCPU_SREG_CS);\n\told_eip = ctxt->_eip;\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\tif (load_segment_descriptor(ctxt, sel, VCPU_SREG_CS))\n\t\treturn X86EMUL_CONTINUE;\n\n\tctxt->_eip = 0;\n\tmemcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);\n\n\tctxt->src.val = old_cs;\n\trc = em_push(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->src.val = old_eip;\n\treturn em_push(ctxt);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t\/* Assignment of RIP may only fail in 64-bit mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t\/* assigning eip failed; restore the old cs *\/\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t\/* Assignment of RIP may only fail in 64-bit mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t\/* assigning eip failed; restore the old cs *\/\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = load_segment_descriptor(ctxt, sel, VCPU_SREG_CS);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->_eip = 0;\n\tmemcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = load_segment_descriptor(ctxt, sel, VCPU_SREG_CS);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->_eip = 0;\n\tmemcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\n\trc = emulate_pop(ctxt, &ctxt->_eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\tif (ctxt->op_bytes == 4)\n\t\tctxt->_eip = (u32)ctxt->_eip;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t\/* Outer-privilege level return is not implemented *\/\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS);\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\n\trc = emulate_pop(ctxt, &ctxt->_eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\tif (ctxt->op_bytes == 4)\n\t\tctxt->_eip = (u32)ctxt->_eip;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t\/* Outer-privilege level return is not implemented *\/\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS);\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip, cs;\n\tu16 old_cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_cs, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t\/* Outer-privilege level return is not implemented *\/\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, 0, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_far(ctxt, eip, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);\n\t\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip, cs;\n\tu16 old_cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_cs, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t\/* Outer-privilege level return is not implemented *\/\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, 0, false,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_far(ctxt, eip, new_desc.l);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);\n\t\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n\t}\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t   u16 selector, int seg)\n{\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\treturn __load_segment_descriptor(ctxt, selector, seg, cpl, false);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t   u16 selector, int seg)\n{\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\treturn __load_segment_descriptor(ctxt, selector, seg, cpl, false);\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t   u16 selector, int seg)\n{\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\treturn __load_segment_descriptor(ctxt, selector, seg, cpl, false, NULL);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t   u16 selector, int seg)\n{\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\treturn __load_segment_descriptor(ctxt, selector, seg, cpl, false, NULL);\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_16 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tctxt->_eip = tss->ip;\n\tctxt->eflags = tss->flag | 2;\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->ax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->cx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->dx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->bx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->sp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->bp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->si;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->di;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\n\tcpl = tss->cs & 3;\n\n\t\/*\n\t * Now load segment descriptors. If fault happens at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_16 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tctxt->_eip = tss->ip;\n\tctxt->eflags = tss->flag | 2;\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->ax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->cx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->dx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->bx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->sp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->bp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->si;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->di;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\n\tcpl = tss->cs & 3;\n\n\t\/*\n\t * Now load segment descriptors. If fault happens at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_16 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tctxt->_eip = tss->ip;\n\tctxt->eflags = tss->flag | 2;\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->ax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->cx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->dx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->bx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->sp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->bp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->si;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->di;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\n\tcpl = tss->cs & 3;\n\n\t\/*\n\t * Now load segment descriptors. If fault happens at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_16 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tctxt->_eip = tss->ip;\n\tctxt->eflags = tss->flag | 2;\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->ax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->cx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->dx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->bx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->sp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->bp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->si;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->di;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\n\tcpl = tss->cs & 3;\n\n\t\/*\n\t * Now load segment descriptors. If fault happens at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_32 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tif (ctxt->ops->set_cr(ctxt, 3, tss->cr3))\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->_eip = tss->eip;\n\tctxt->eflags = tss->eflags | 2;\n\n\t\/* General purpose registers *\/\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->eax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->ecx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->edx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->ebx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->esp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->ebp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->esi;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->edi;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors.  This is important because CPL checks will\n\t * use CS.RPL.\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt_selector, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\tset_segment_selector(ctxt, tss->fs, VCPU_SREG_FS);\n\tset_segment_selector(ctxt, tss->gs, VCPU_SREG_GS);\n\n\t\/*\n\t * If we're switching between Protected Mode and VM86, we need to make\n\t * sure to update the mode before loading the segment descriptors so\n\t * that the selectors are interpreted correctly.\n\t *\/\n\tif (ctxt->eflags & X86_EFLAGS_VM) {\n\t\tctxt->mode = X86EMUL_MODE_VM86;\n\t\tcpl = 3;\n\t} else {\n\t\tctxt->mode = X86EMUL_MODE_PROT32;\n\t\tcpl = tss->cs & 3;\n\t}\n\n\t\/*\n\t * Now load segment descriptors. If fault happenes at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR,\n\t\t\t\t\tcpl, true, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_32 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tif (ctxt->ops->set_cr(ctxt, 3, tss->cr3))\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->_eip = tss->eip;\n\tctxt->eflags = tss->eflags | 2;\n\n\t\/* General purpose registers *\/\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->eax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->ecx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->edx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->ebx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->esp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->ebp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->esi;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->edi;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors.  This is important because CPL checks will\n\t * use CS.RPL.\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt_selector, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\tset_segment_selector(ctxt, tss->fs, VCPU_SREG_FS);\n\tset_segment_selector(ctxt, tss->gs, VCPU_SREG_GS);\n\n\t\/*\n\t * If we're switching between Protected Mode and VM86, we need to make\n\t * sure to update the mode before loading the segment descriptors so\n\t * that the selectors are interpreted correctly.\n\t *\/\n\tif (ctxt->eflags & X86_EFLAGS_VM) {\n\t\tctxt->mode = X86EMUL_MODE_VM86;\n\t\tcpl = 3;\n\t} else {\n\t\tctxt->mode = X86EMUL_MODE_PROT32;\n\t\tcpl = tss->cs & 3;\n\t}\n\n\t\/*\n\t * Now load segment descriptors. If fault happenes at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR,\n\t\t\t\t\tcpl, true, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl,\n\t\t\t\t\ttrue, NULL);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_32 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tif (ctxt->ops->set_cr(ctxt, 3, tss->cr3))\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->_eip = tss->eip;\n\tctxt->eflags = tss->eflags | 2;\n\n\t\/* General purpose registers *\/\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->eax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->ecx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->edx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->ebx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->esp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->ebp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->esi;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->edi;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors.  This is important because CPL checks will\n\t * use CS.RPL.\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt_selector, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\tset_segment_selector(ctxt, tss->fs, VCPU_SREG_FS);\n\tset_segment_selector(ctxt, tss->gs, VCPU_SREG_GS);\n\n\t\/*\n\t * If we're switching between Protected Mode and VM86, we need to make\n\t * sure to update the mode before loading the segment descriptors so\n\t * that the selectors are interpreted correctly.\n\t *\/\n\tif (ctxt->eflags & X86_EFLAGS_VM) {\n\t\tctxt->mode = X86EMUL_MODE_VM86;\n\t\tcpl = 3;\n\t} else {\n\t\tctxt->mode = X86EMUL_MODE_PROT32;\n\t\tcpl = tss->cs & 3;\n\t}\n\n\t\/*\n\t * Now load segment descriptors. If fault happenes at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"static int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t struct tss_segment_32 *tss)\n{\n\tint ret;\n\tu8 cpl;\n\n\tif (ctxt->ops->set_cr(ctxt, 3, tss->cr3))\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->_eip = tss->eip;\n\tctxt->eflags = tss->eflags | 2;\n\n\t\/* General purpose registers *\/\n\t*reg_write(ctxt, VCPU_REGS_RAX) = tss->eax;\n\t*reg_write(ctxt, VCPU_REGS_RCX) = tss->ecx;\n\t*reg_write(ctxt, VCPU_REGS_RDX) = tss->edx;\n\t*reg_write(ctxt, VCPU_REGS_RBX) = tss->ebx;\n\t*reg_write(ctxt, VCPU_REGS_RSP) = tss->esp;\n\t*reg_write(ctxt, VCPU_REGS_RBP) = tss->ebp;\n\t*reg_write(ctxt, VCPU_REGS_RSI) = tss->esi;\n\t*reg_write(ctxt, VCPU_REGS_RDI) = tss->edi;\n\n\t\/*\n\t * SDM says that segment selectors are loaded before segment\n\t * descriptors.  This is important because CPL checks will\n\t * use CS.RPL.\n\t *\/\n\tset_segment_selector(ctxt, tss->ldt_selector, VCPU_SREG_LDTR);\n\tset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\n\tset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\n\tset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\n\tset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\n\tset_segment_selector(ctxt, tss->fs, VCPU_SREG_FS);\n\tset_segment_selector(ctxt, tss->gs, VCPU_SREG_GS);\n\n\t\/*\n\t * If we're switching between Protected Mode and VM86, we need to make\n\t * sure to update the mode before loading the segment descriptors so\n\t * that the selectors are interpreted correctly.\n\t *\/\n\tif (ctxt->eflags & X86_EFLAGS_VM) {\n\t\tctxt->mode = X86EMUL_MODE_VM86;\n\t\tcpl = 3;\n\t} else {\n\t\tctxt->mode = X86EMUL_MODE_PROT32;\n\t\tcpl = tss->cs & 3;\n\t}\n\n\t\/*\n\t * Now load segment descriptors. If fault happenes at this stage\n\t * it is handled in a context of new task\n\t *\/\n\tret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\tret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl, true);\n\tif (ret != X86EMUL_CONTINUE)\n\t\treturn ret;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-3647",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel through 3.17.2 does not properly perform RIP changes, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 ||\n\t    (mode >= X86EMUL_MODE_PROT16 && (ctxt->modrm & 0x80)))) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t\t     (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64 && (ctxt->d & Stack))\n\t\t\tctxt->op_bytes = 8;\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea += ctxt->_eip;\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-8481",
    "description":"[{'lang': 'en', 'value': 'The instruction decoder in arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel before 3.18-rc2 does not properly handle invalid instructions, which allows guest OS users to cause a denial of service (NULL pointer dereference and host OS crash) via a crafted application that triggers (1) an improperly fetched instruction or (2) an instruction that occupies too many bytes.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-8480.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 ||\n\t    (mode >= X86EMUL_MODE_PROT16 && (ctxt->modrm & 0x80)))) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t\t     (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64 && (ctxt->d & Stack))\n\t\t\tctxt->op_bytes = 8;\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea += ctxt->_eip;\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2014-8481",
    "description":"[{'lang': 'en', 'value': 'The instruction decoder in arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel before 3.18-rc2 does not properly handle invalid instructions, which allows guest OS users to cause a denial of service (NULL pointer dereference and host OS crash) via a crafted application that triggers (1) an improperly fetched instruction or (2) an instruction that occupies too many bytes.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-8480.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 ||\n\t    (mode >= X86EMUL_MODE_PROT16 && (ctxt->modrm & 0x80)))) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t\t     (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64 && (ctxt->d & Stack))\n\t\t\tctxt->op_bytes = 8;\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\ndone:\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea += ctxt->_eip;\n\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-8481",
    "description":"[{'lang': 'en', 'value': 'The instruction decoder in arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel before 3.18-rc2 does not properly handle invalid instructions, which allows guest OS users to cause a denial of service (NULL pointer dereference and host OS crash) via a crafted application that triggers (1) an improperly fetched instruction or (2) an instruction that occupies too many bytes.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-8480.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 ||\n\t    (mode >= X86EMUL_MODE_PROT16 && (ctxt->modrm & 0x80)))) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t\t     (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64 && (ctxt->d & Stack))\n\t\t\tctxt->op_bytes = 8;\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\ndone:\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea += ctxt->_eip;\n\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2014-8481",
    "description":"[{'lang': 'en', 'value': 'The instruction decoder in arch\/x86\/kvm\/emulate.c in the KVM subsystem in the Linux kernel before 3.18-rc2 does not properly handle invalid instructions, which allows guest OS users to cause a denial of service (NULL pointer dereference and host OS crash) via a crafted application that triggers (1) an improperly fetched instruction or (2) an instruction that occupies too many bytes.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2014-8480.'}]"
  },
  {
    "text":"static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\t\/* inject #GP if in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL)\n\t\treturn emulate_gp(ctxt, 0);\n\n\t\/*\n\t * Not recognized on AMD in compat mode (but is recognized in legacy\n\t * mode).\n\t *\/\n\tif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\n\t    && !vendor_intel(ctxt))\n\t\treturn emulate_ud(ctxt);\n\n\t\/* sysenter\/sysexit have not been tested in 64bit mode. *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tif ((msr_data & 0xfffc) == 0x0)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n\tcs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;\n\tss_sel = cs_sel + 8;\n\tif (efer & EFER_LMA) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n\tctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\n\t\t\t\t\t\t\t      (u32)msr_data;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2015-0239",
    "description":"[{'lang': 'en', 'value': 'The em_sysenter function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.'}]"
  },
  {
    "text":"static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\t\/* inject #GP if in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL)\n\t\treturn emulate_gp(ctxt, 0);\n\n\t\/*\n\t * Not recognized on AMD in compat mode (but is recognized in legacy\n\t * mode).\n\t *\/\n\tif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\n\t    && !vendor_intel(ctxt))\n\t\treturn emulate_ud(ctxt);\n\n\t\/* sysenter\/sysexit have not been tested in 64bit mode. *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tif ((msr_data & 0xfffc) == 0x0)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n\tcs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;\n\tss_sel = cs_sel + 8;\n\tif (efer & EFER_LMA) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n\tctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\n\t\t\t\t\t\t\t      (u32)msr_data;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2015-0239",
    "description":"[{'lang': 'en', 'value': 'The em_sysenter function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.'}]"
  },
  {
    "text":"static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\t\/* inject #GP if in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL)\n\t\treturn emulate_gp(ctxt, 0);\n\n\t\/*\n\t * Not recognized on AMD in compat mode (but is recognized in legacy\n\t * mode).\n\t *\/\n\tif ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)\n\t    && !vendor_intel(ctxt))\n\t\treturn emulate_ud(ctxt);\n\n\t\/* sysenter\/sysexit have not been tested in 64bit mode. *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (ctxt->mode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n\tcs_sel = (u16)msr_data;\n\tcs_sel &= ~SELECTOR_RPL_MASK;\n\tss_sel = cs_sel + 8;\n\tss_sel &= ~SELECTOR_RPL_MASK;\n\tif (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n\tctxt->_eip = msr_data;\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2015-0239",
    "description":"[{'lang': 'en', 'value': 'The em_sysenter function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.'}]"
  },
  {
    "text":"static int em_sysenter(struct x86_emulate_ctxt *ctxt)\n{\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tstruct desc_struct cs, ss;\n\tu64 msr_data;\n\tu16 cs_sel, ss_sel;\n\tu64 efer = 0;\n\n\tops->get_msr(ctxt, MSR_EFER, &efer);\n\t\/* inject #GP if in real mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_REAL)\n\t\treturn emulate_gp(ctxt, 0);\n\n\t\/*\n\t * Not recognized on AMD in compat mode (but is recognized in legacy\n\t * mode).\n\t *\/\n\tif ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)\n\t    && !vendor_intel(ctxt))\n\t\treturn emulate_ud(ctxt);\n\n\t\/* sysenter\/sysexit have not been tested in 64bit mode. *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tsetup_syscalls_segments(ctxt, &cs, &ss);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\n\tswitch (ctxt->mode) {\n\tcase X86EMUL_MODE_PROT32:\n\t\tif ((msr_data & 0xfffc) == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT64:\n\t\tif (msr_data == 0x0)\n\t\t\treturn emulate_gp(ctxt, 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tctxt->eflags &= ~(EFLG_VM | EFLG_IF);\n\tcs_sel = (u16)msr_data;\n\tcs_sel &= ~SELECTOR_RPL_MASK;\n\tss_sel = cs_sel + 8;\n\tss_sel &= ~SELECTOR_RPL_MASK;\n\tif (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {\n\t\tcs.d = 0;\n\t\tcs.l = 1;\n\t}\n\n\tops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\n\tops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\n\tctxt->_eip = msr_data;\n\n\tops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\n\t*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;\n\n\treturn X86EMUL_CONTINUE;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2015-0239",
    "description":"[{'lang': 'en', 'value': 'The em_sysenter function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 3.18.5, when the guest OS lacks SYSENTER MSR initialization, allows guest OS users to gain guest OS privileges or cause a denial of service (guest OS crash) by triggering use of a 16-bit code segment for emulation of a SYSENTER instruction.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative && likely(ctxt->memopp))\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2016-8630",
    "description":"[{'lang': 'en', 'value': 'The x86_decode_insn function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.7, when KVM is enabled, allows local users to cause a denial of service (host OS crash) via a certain use of a ModR\/M byte in an undefined instruction.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative && likely(ctxt->memopp))\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2016-8630",
    "description":"[{'lang': 'en', 'value': 'The x86_decode_insn function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.7, when KVM is enabled, allows local users to cause a denial of service (host OS crash) via a certain use of a ModR\/M byte in an undefined instruction.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2016-8630",
    "description":"[{'lang': 'en', 'value': 'The x86_decode_insn function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.7, when KVM is enabled, allows local users to cause a denial of service (host OS crash) via a certain use of a ModR\/M byte in an undefined instruction.'}]"
  },
  {
    "text":"int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t\/* Legacy prefixes. *\/\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t\/* operand-size override *\/\n\t\t\top_prefix = true;\n\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t\/* address-size override *\/\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t\/* switch between 4\/8 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t\/* switch between 2\/4 bytes *\/\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t\/* ES override *\/\n\t\tcase 0x2e:\t\/* CS override *\/\n\t\tcase 0x36:\t\/* SS override *\/\n\t\tcase 0x3e:\t\/* DS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t\/* FS override *\/\n\t\tcase 0x65:\t\/* GS override *\/\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: \/* REX *\/\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t\/* LOCK *\/\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t\/* REPNE\/REPNZ *\/\n\t\tcase 0xf3:\t\/* REP\/REPE\/REPZ *\/\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t\/* Any legacy prefix after a REX prefix nullifies its effect. *\/\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t\/* REX prefix. *\/\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t\/* REX.W *\/\n\n\t\/* Opcode byte(s). *\/\n\topcode = opcode_table[ctxt->b];\n\t\/* Two-byte opcode? *\/\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t\/* 0F_38 opcode map *\/\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t\/* vex-prefix instructions are not implemented *\/\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t\/* Unrecognised? *\/\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t\/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t *\/\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t\/* ModRM and SIB bytes. *\/\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t\/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t *\/\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t\/* Decode and fetch the destination operand: register or memory. *\/\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2016-8630",
    "description":"[{'lang': 'en', 'value': 'The x86_decode_insn function in arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.7, when KVM is enabled, allows local users to cause a denial of service (host OS crash) via a certain use of a ModR\/M byte in an undefined instruction.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t\/* Assignment of RIP may only fail in 64-bit mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t\/* assigning eip failed; restore the old cs *\/\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2016-9756",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t\/* Assignment of RIP may only fail in 64-bit mode *\/\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t\/* assigning eip failed; restore the old cs *\/\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
    "vulnerable":1,
    "cve_id":"CVE-2016-9756",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\tstruct desc_struct new_desc;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\t\/* Error handling is not implemented. *\/\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2016-9756",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.'}]"
  },
  {
    "text":"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\tstruct desc_struct new_desc;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\t\/* Error handling is not implemented. *\/\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\treturn rc;\n}",
    "vulnerable":0,
    "cve_id":"CVE-2016-9756",
    "description":"[{'lang': 'en', 'value': 'arch\/x86\/kvm\/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.'}]"
  }
]